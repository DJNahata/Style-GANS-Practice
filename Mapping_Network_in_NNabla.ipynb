{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Mapping Network in NNabla.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNuod9kSbMeIwoYZjXnU4W7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DJNahata/Style-GANS-Practice/blob/master/Mapping_Network_in_NNabla.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "01z2xTeIiW84",
        "colab_type": "code",
        "outputId": "61e16488-1b44-43da-93d3-d55a8047427a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 608
        }
      },
      "source": [
        "#Installing Nnabla\n",
        "!pip install nnabla"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting nnabla\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6f/29/e2a05e0c9f0f9f3647fcd4a04bc167bb8bf85d8246581951f0dd3c206457/nnabla-1.7.0-cp36-cp36m-manylinux1_x86_64.whl (14.1MB)\n",
            "\u001b[K     |████████████████████████████████| 14.2MB 232kB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nnabla) (1.12.0)\n",
            "Requirement already satisfied: Cython in /usr/local/lib/python3.6/dist-packages (from nnabla) (0.29.16)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from nnabla) (46.1.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from nnabla) (1.18.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from nnabla) (2.21.0)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (from nnabla) (7.0.0)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from nnabla) (1.12.40)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from nnabla) (1.4.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from nnabla) (2.10.0)\n",
            "Requirement already satisfied: protobuf>=3.6 in /usr/local/lib/python3.6/dist-packages (from nnabla) (3.10.0)\n",
            "Requirement already satisfied: imageio in /usr/local/lib/python3.6/dist-packages (from nnabla) (2.4.1)\n",
            "Collecting onnx\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/f4/e126b60d109ad1e80020071484b935980b7cce1e4796073aab086a2d6902/onnx-1.6.0-cp36-cp36m-manylinux1_x86_64.whl (4.8MB)\n",
            "\u001b[K     |████████████████████████████████| 4.8MB 34.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from nnabla) (4.38.0)\n",
            "Collecting configparser\n",
            "  Downloading https://files.pythonhosted.org/packages/4b/6b/01baa293090240cf0562cc5eccb69c6f5006282127f2b846fad011305c79/configparser-5.0.0-py3-none-any.whl\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from nnabla) (3.13)\n",
            "Requirement already satisfied: contextlib2 in /usr/local/lib/python3.6/dist-packages (from nnabla) (0.5.5)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->nnabla) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->nnabla) (2020.4.5.1)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->nnabla) (2.8)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->nnabla) (1.24.3)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->nnabla) (0.3.3)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->nnabla) (0.9.5)\n",
            "Requirement already satisfied: botocore<1.16.0,>=1.15.40 in /usr/local/lib/python3.6/dist-packages (from boto3->nnabla) (1.15.40)\n",
            "Requirement already satisfied: typing-extensions>=3.6.2.1 in /usr/local/lib/python3.6/dist-packages (from onnx->nnabla) (3.6.6)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.40->boto3->nnabla) (0.15.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.40->boto3->nnabla) (2.8.1)\n",
            "Installing collected packages: onnx, configparser, nnabla\n",
            "Successfully installed configparser-5.0.0 nnabla-1.7.0 onnx-1.6.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vufQKQDZmg3l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Importing the Pre-Trained weights as the List.. using pickle\n",
        "# Dense Layer Weights\n",
        "import pickle\n",
        "with open(\"G_mapping_weigh.txt\", \"rb\") as fp:   \n",
        "   G_mapping_weights = pickle.load(fp)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Bak8M87lW-o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Bias Layer Weights\n",
        "with open(\"G_mapping_bi.txt\", \"rb\") as fp:   \n",
        "   G_mapping_bias = pickle.load(fp)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0W4yIBJcCZ5G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Latent Input Vector z which is kept same\n",
        "with open(\"latent_z.txt\", 'rb') as fp:\n",
        "   latent_z = pickle.load(fp)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "153A9tyVm3hH",
        "colab_type": "code",
        "outputId": "11a9d9b1-474d-474d-c7e1-2d1a88a82446",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "import numpy as np\n",
        "import nnabla as nn\n",
        "import nnabla.parametric_functions as PF\n",
        "import nnabla.functions as F\n",
        "import nnabla.solvers as S\n",
        "import nnabla.logger as logger\n",
        "import nnabla.utils.save as save\n",
        "from nnabla.parameter import get_parameter_or_create\n",
        "from nnabla.ext_utils import get_extension_context\n",
        "from nnabla.parametric_functions import parametric_function_api\n",
        "import nnabla.initializer as I\n",
        "nn.set_auto_forward(True)\n",
        "print(\"DJ1\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-04-22 06:54:12,475 [nnabla][INFO]: Initializing CPU extension...\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "DJ1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wNFnBLOHfbwo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_weight(he_std, gain = np.sqrt(2), use_wscale = False, lrmul = 1):\n",
        "    if use_wscale:\n",
        "        init_std = 1.0 / lrmul\n",
        "        runtime_coeff = he_std * lrmul\n",
        "    else: \n",
        "        init_std = he_std / lrmul\n",
        "        runtime_coeff = lrmul\n",
        "\n",
        "    w_init = I.NormalInitializer(sigma = init_std)   #I.NormalInitializer function is used with sigma argument as the standard deviation, so as to get the weights of these variables \n",
        "    return w_init, runtime_coeff"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "07z6bmjzffD3",
        "colab_type": "code",
        "outputId": "023ff7fe-6685-4c84-e60d-9ed5458f6594",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "from __future__ import absolute_import\n",
        "from six.moves import range\n",
        "print(\"DJ1\")\n",
        "\n",
        "import functools\n",
        "# Globally prepared params.\n",
        "w = nn.parameter.get_parameter_or_create('BLURRING_FILTER', shape=(3,3), need_grad=False)  #why and how is it used for the parameter\n",
        "w.d = np.array([[1., 2., 1.], [2., 4., 2.], [1., 2., 1.]])   \n",
        "\n",
        "print('')\n",
        "\n",
        "\n",
        "def lerp(a, b, t):\n",
        "    return ((a + (b - a) * t))\n",
        "\n",
        "def lerp_clip(a, b, t):\n",
        "    return (a + (b - a) * F.clip_by_value(t, 0.0, 1.0))\n",
        "\n",
        "\n",
        "#Computing the mean across the feature maps i.e. axis 2 and 3 for H and W with data format of x as NCHW\n",
        "#--------------------\n",
        "def compute_channel_mean(x, keepdims = True):\n",
        "    ch_mean = F.mean(x, axis = [2,3], keepdims = keepdims)\n",
        "    return ch_mean\n",
        "\n",
        "#--------------------\n",
        "\n",
        "\n",
        "#-------------------------------\n",
        "#Computing the standard deviation across the feature maps i.e. axis 2 and 3 for H and W with data format of x as NCHW\n",
        "def compute_channel_std(x, keepdims = True):\n",
        "    x1 = compute_channel_mean(x)\n",
        "    var = F.mean(F.pow_scalar((x - x1), 2.0), axis = [2,3], keepdims = keepdims)\n",
        "    std = F.pow_scalar(var, 0.5)\n",
        "    return std\n",
        "\n",
        "#-------------------------------\n",
        "\n",
        "\n",
        "#---------------------------------------------------\n",
        "#Adaptive Instance Normalization to be used\n",
        "def adain(x, y):\n",
        "    y_s, y_b = y\n",
        "    ch1_mean = compute_channel_mean(x)\n",
        "    ch1_std = compute_channel_std(x)\n",
        "    x = (x - ch1_mean) / ch1_std\n",
        "    a1 = (y_s * x) + y_b\n",
        "    return a1 \n",
        "\n",
        "#-------------------------------\n",
        "\n",
        "\n",
        "#New blur function to be used ------------------------------------------\n",
        "def blur2d(x, f=[1,2,1], normalize = True, flip = False, stride = 1):\n",
        "    ## assert x.ndim == 4 and all(dim.value is not None for dim in x.shape[1:])\n",
        "    assert isinstance(stride, int) and stride >= 1\n",
        "\n",
        "    f = np.array(f, dtype = np.float32)\n",
        "\n",
        "    if f.ndim == 1:\n",
        "        f = f[:, np.newaxis] * f[np.newaxis, :]\n",
        "\n",
        "    if flip:\n",
        "        f = f[::-1, ::-1]\n",
        "\n",
        "    if normalize:\n",
        "        f = f / np.sum(f)\n",
        "\n",
        "    f = f[ np.newaxis, :, :]\n",
        "    print(\"F.shape is:\", f.shape)\n",
        "    f = np.tile(f, [int(x.shape[1]), 1, 1])\n",
        "    print(\"F.shape is:\",f.shape)\n",
        "    if f.shape == (1, 1) and f[0,0] == 1:\n",
        "        return x\n",
        "\n",
        "    #orig_dtype = x.dtype\n",
        "    strides = (stride, stride) \n",
        "    w = nn.Variable.from_numpy_array(f)\n",
        "    print(w.shape)\n",
        "    padding = (w.shape[1] // 2, w.shape[2] // 2)\n",
        "\n",
        "    y = F.depthwise_convolution(x, w, pad = padding, stride = strides)\n",
        "    return y"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "DJ1\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SA1O70RYfh5F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Upscale operation function to be used \n",
        "def upscale2d(x, factor = 2, gain = 1):\n",
        "    # to check whether the shape of the x is 4 or not\n",
        "    #assert x.ndim == 4 and all(dim.value is not None for dim in x.shape[1:])\n",
        "    assert isinstance(factor, int) and factor >=1 \n",
        "\n",
        "    if gain!=1:\n",
        "        x = x * gain\n",
        "    \n",
        "    if factor == 1:\n",
        "        return x\n",
        "\n",
        "    s = x.shape\n",
        "    #reshape and tile function similar as that of the tensorflow\n",
        "    x = F.reshape(x, [-1, s[1], s[2], 1, s[3], 1])\n",
        "    x = F.tile(x, [1, 1, 1, factor, 1, factor])\n",
        "    x = F.reshape(x, [-1, s[1], s[2] * factor, s[3] * factor])\n",
        "    return x\n",
        "    \n",
        "#--------------------------------------------------------------\n",
        "\n",
        "\n",
        "#----------------------------------------------------------------------\n",
        "#downscale2d operation to be used \n",
        "\n",
        "def downscale2d(x, factor = 2, gain = 1):\n",
        "    #assert x.ndim == 4 and all(dim.value is not None for dim in x.shape[1:])\n",
        "    assert isinstance(factor, int) and factor >=1\n",
        "\n",
        "    if factor == 2:\n",
        "        f = [np.sqrt(gain) / factor] * factor\n",
        "        f = np.array(f, dtype = np.float32)\n",
        "        if f.ndim == 1:\n",
        "            f = f[:, np.newaxis] * f[np.newaxis, :]\n",
        "        assert f.ndim == 2\n",
        "\n",
        "        f = f[np.newaxis, :, :]\n",
        "        f = np.tile(f, [int(x.shape[1]), 1, 1])\n",
        "        w = nn.Variable.from_numpy_array(f)\n",
        "        strides = (factor, factor)\n",
        "        padding = (w.shape[1] // 2, w.shape[2] // 2)\n",
        "        \n",
        "        y = F.depthwise_convolution(x, w, stride = strides)\n",
        "        return y \n",
        "    \n",
        "    if gain != 1:\n",
        "        x = x * gain\n",
        "\n",
        "    if factor == 1:\n",
        "        return x\n",
        "\n",
        "    kernel = [factor, factor]\n",
        "    y = F.average_pooling(x, kernel = kernel, stride = kernel)    # a bit of doubt regarding this statement\n",
        "    return y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zR-6R4UFfl_i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def minibatch_stddev_layer(x, group_size=4, num_new_features=1):\n",
        "    group_size = min(group_size, x.shape[0])     # Minibatch must be divisible by (or smaller than) group_size.\n",
        "    s = x.shape                                             # [NCHW]  Input shape.\n",
        "    y = F.reshape(x, [group_size, -1, num_new_features, s[1]//num_new_features, s[2], s[3]])   # [GMncHW] Split minibatch into M groups of size G. Split channels into n channel groups c.\n",
        "    y -= F.mean(y, axis=0, keepdims=True)           # [GMncHW] Subtract mean over group.\n",
        "    y = F.mean(F.pow_scalar(y, 2.0), axis=0)                # [MncHW]  Calc variance over group.\n",
        "    y = F.add_scalar(F.pow_scalar(y, 2.0) + 1e-8)                                   # [MncHW]  Calc stddev over group.\n",
        "    y = F.mean(y, axis=[2,3,4], keepdims=True)      # [Mn111]  Take average over fmaps and pixels.\n",
        "    y = F.mean(y, axis=[2])                         # [Mn11] Split channels into c channel groups\n",
        "    y = F.tile(y, [group_size, 1, s[2], s[3]])             # [NnHW]  Replicate over group and pixels.\n",
        "    return F.concatenate(x, y, axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wb4X-AGbfo8w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def dense(x, fmaps, gain = np.sqrt(2), use_wscale = False, lrmul = 1, **kwargs):\n",
        "    he_std = I.calc_normal_std_he_forward(x.shape[1], fmaps)\n",
        "    w_init, runtime_coeff = get_weight(he_std, gain, use_wscale, lrmul, **kwargs)\n",
        "    w = nn.parameter.get_parameter_or_create('Dense', shape = (x.shape[1], fmaps), initializer = w_init, need_grad = True)\n",
        "    w = w * runtime_coeff\n",
        "    return (F.affine(x, w, bias=None, base_axis=1))\n",
        "\n",
        "\n",
        "def conv2d(x, fmaps, kernel, gain = np.sqrt(2), use_wscale = False, **kwargs):\n",
        "    assert isinstance(kernel, int) or isinstance(kernel, float)\n",
        "    assert kernel >= 1 and kernel % 2 == 1\n",
        "    if kernel == 1:\n",
        "        padding = None\n",
        "    else:\n",
        "        padding = ((kernel - 2), (kernel - 2))  #tuple of padding\n",
        "    kernel_tuple = (kernel, kernel)  #tuple of kernel_tuple\n",
        "    he_std = I.calc_normal_std_he_forward(fmaps, x.shape[1], kernel_tuple)        # (kernel, kernel, x.shape[1], fmaps)\n",
        "    w_init, runtime_coeff = get_weight(he_std, gain, use_wscale, **kwargs)\n",
        "    return PF.convolution(x, fmaps, kernel = kernel_tuple, w_init = w_init, with_bias = False, pad = padding, apply_w = lambda w: w * runtime_coeff)\n",
        "\n",
        "\n",
        "def upscale2d_conv2d(x, fmaps, kernel, gain = np.sqrt(2), use_wscale = False, fused_scale = 'auto', **kwargs):\n",
        "    assert kernel >= 1 and kernel % 2 == 1\n",
        "    assert fused_scale in [True, False, 'auto']\n",
        "    if fused_scale == 'auto':\n",
        "        fused_scale = min(x.shape[2:]) * 2 >= 128\n",
        "\n",
        "    if not fused_scale:\n",
        "        return conv2d(upscale2d(x), fmaps, kernel, **kwargs)\n",
        "\n",
        "    kernel_tuple = (kernel, kernel)\n",
        "    he_std = I.calc_normal_std_he_forward(x.shape[1], fmaps, kernel_tuple)        # (kernel, kernel, x.shape[1], fmaps)\n",
        "    w_init, runtime_coeff = get_weight(he_std, gain, use_wscale, **kwargs)\n",
        "    w = nn.parameter.get_parameter_or_create('deconv/W', shape = (x.shape[1], fmaps, kernel, kernel), initializer = w_init, need_grad = True)\n",
        "    w = w * runtime_coeff\n",
        "    w = F.pad(w, (1, 1, 1, 1), mode = 'constant')\n",
        "    w = w[..., 1:, 1:] + w[..., :-1, 1:] + w[..., 1:, :-1] + w[..., :-1, :-1]\n",
        "    kernel_size = w.shape[-1]\n",
        "    padding = ((kernel_size - 2) / 2, (kernel_size - 2) / 2)\n",
        "    stride = (2 , 2)\n",
        "    return F.deconvolution(x, w, bias = None, pad = padding, stride = stride)\n",
        "\n",
        "\n",
        "def conv2d_downscale2d(x, fmaps, kernel, gain = np.sqrt(2), use_wscale = False, fused_scale = 'auto', **kwargs):\n",
        "    #assert kernel % 2 == 1 and kernel >= 1\n",
        "    assert fused_scale in [True, False, 'auto']\n",
        "    if fused_scale == 'auto':\n",
        "        fused_scale = min(x.shape[2:]) >= 128   #why is this written and what is the condition behind that\n",
        "\n",
        "    if not fused_scale:\n",
        "        return downscale2d(conv2d(x, fmaps, kernel, **kwargs))\n",
        "\n",
        "    kernel_tuple = (kernel, kernel)\n",
        "    he_std = I.calc_normal_std_he_forward(fmaps, x.shape[1], kernel_tuple)\n",
        "    w_init, runtime_coeff = get_weight(he_std, gain = gain, use_wscale = use_wscale, **kwargs)\n",
        "    w = nn.parameter.get_parameter_or_create('conv/W', shape = (fmaps, x.shape[1], kernel, kernel), initializer = w_init, need_grad = True)\n",
        "    w = w * runtime_coeff\n",
        "    w = F.pad(w, (1, 1, 1, 1), mode = 'constant')\n",
        "    w = w[..., 1:, 1:] + w[..., :-1, 1:] + w[..., 1:, :-1] + w[..., :-1, :-1]\n",
        "    w = w * 0.25\n",
        "    kernel_size = w.shape[-1]\n",
        "    padding = ((kernel_size - 2) / 2, (kernel_size - 2) / 2)\n",
        "    stride = (2,2)\n",
        "    return F.convolution(x, w, bias=None, pad = padding, stride = stride)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vr4S72Wzft5W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#--------------------------------------------------------------\n",
        "#Apply Bias\n",
        "def apply_bias(x, lrmul = 1):\n",
        "    b = nn.parameter.get_parameter_or_create(name = 'bias', shape = [x.shape[1]], initializer = I.ConstantInitializer(0))   \n",
        "    b = b * lrmul\n",
        "    if x.ndim == 2:\n",
        "        print(\"DJ\")\n",
        "        return ((x + F.reshape(b, [1, -1])))\n",
        "    return ((x + F.reshape(b, [1, -1, 1, 1]))) \n",
        "\n",
        "#--------------------------------------------------------------"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GHIDRUvnpkyk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#--------------------------------------\n",
        "#application of the activation function\n",
        "def leaky_relu(x):\n",
        "    return functools.partial(F.leaky_relu, alpha = 0.2)(x)\n",
        "#--------------------------------------"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LHUlGnUG0tPr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#---------------------------------------------\n",
        "# defining the pixel normalization which is probably used in the Discriminator network\n",
        "def pixel_norm(x, epsilon = 1e-8):\n",
        "    mean1 = F.mean(F.pow_scalar(x, 2.0), axis = 1, keepdims = True)\n",
        "    new_mean1 = F.add_scalar(mean1, epsilon)\n",
        "    denominator = F.pow_scalar(new_mean1, 0.5)\n",
        "    x = F.div2(x, denominator)\n",
        "    return x\n",
        "\n",
        "#--------------------------------------------------"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xp1rdqMwBq91",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#---------------------------------------------------------------\n",
        "#Defining the Instance Normalization which is used in the Synthesis Generator Network with the format NCHW of the x   Change this FUnction i.e. Instance Normalization..\n",
        "def instance_norm(x, epsilon = 1e-8):\n",
        "    assert len(x.shape) == 4\n",
        "    with nn.parameter_scope('InstanceNorm'):\n",
        "        x1 = compute_channel_mean(x)\n",
        "        x = x - x1\n",
        "        x2 = compute_channel_mean(F.add_scalar(F.pow_scalar(x , 2.0), epsilon))\n",
        "        x2 = F.pow_scalar(x2, 0.5)\n",
        "        x = x / x2\n",
        "        return x\n",
        "\n",
        "#-----------------------------------------------------------------\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OTb4UfiLf6F5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#-------------------------------------------------------\n",
        "#defining the style modulation i.e. combining ys and yb with the resultant of the addition of the noise and the feature maps\n",
        "def style_mod(x, dlatent, **kwargs):\n",
        "    with nn.parameter_scope('StyleMod'):\n",
        "        style = apply_bias(dense(dlatent, fmaps = x.shape[1] * 2, **kwargs))\n",
        "        style = F.reshape(style, [-1, 2, x.shape[1]] + [1] * (len(x.shape) - 2))\n",
        "        return x * (style[:,0] + 1) + style[:,1]\n",
        "\n",
        "#-------------------------------------------------------------------------------"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t-FhjJyDf9ZO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#-------------------------------------------\n",
        "#starting of the Mapping Network\n",
        "def G_mapping(\n",
        "    latents_in,  # First input: Latent vectors (Z)  [minibatch, latent_size]\n",
        "    labels_in,   # Second input: Conditioning labels  [minibatch, label_size]\n",
        "    latent_size = 512,   # Latent vector (Z) dimensionality\n",
        "    label_size = 0,     # Label Dimensionality is 0, therefore no labels are present\n",
        "    dlatent_size = 512, # Disentangled latent (W) dimensionality \n",
        "    dlatent_broadcast = 14,  # Output Disentangled latent (W) as [minibatch, dlatent_size] or [minibatch, dlatent_broadcast, dlatent_size]\n",
        "    mapping_layers = 8,   # Number of Mapping Layers in the G_Mapping Network\n",
        "    mapping_fmaps = 512,  # Number of activations in the mapping layers.\n",
        "    mapping_lrmul = 0.01, # Learning rate multiplier for the mapping layers\n",
        "    mapping_nonlinearity = 'lrelu',   # Activation function:  'relu', 'lrelu'\n",
        "    use_wscale = True,    # Enable equalized Learning Rate\n",
        "    normalize_latents = True,  # Normalizing the Latents before feeding them to the mapping Layers\n",
        "    dtype = 'float32',  # Data type to use for activation and outputs\n",
        "    **_kwargs\n",
        "    ):\n",
        "  \n",
        "    act, gain = {'relu' : (F.relu, np.sqrt(2)), 'lrelu' : (leaky_relu, np.sqrt(2))}[mapping_nonlinearity]\n",
        "\n",
        "    x = latents_in\n",
        "\n",
        "    if label_size:\n",
        "        with nn.parameter_scope('LabelConcat'):\n",
        "            y = PF.affine(labels_in, latent_size)\n",
        "            x = nn.concatenate(x, y, axis = 1)\n",
        "\n",
        "    if normalize_latents:\n",
        "        x = pixel_norm(x)\n",
        "\n",
        "\n",
        "    with nn.parameter_scope('G_mapping'):\n",
        "        for layer_idx in range(mapping_layers):\n",
        "            scope_name = 'FC_{}'.format(layer_idx) \n",
        "            with nn.parameter_scope(scope_name):\n",
        "                if layer_idx == mapping_layers:\n",
        "                    famps = dlatent_size\n",
        "                else:\n",
        "                    fmaps = mapping_fmaps\n",
        "                x = dense(x, fmaps, use_wscale=use_wscale, lrmul=mapping_lrmul)\n",
        "                x = apply_bias(x, lrmul=mapping_lrmul)\n",
        "                x = act(x)\n",
        "\n",
        "\n",
        "    if dlatent_broadcast is not None:           \n",
        "        with nn.parameter_scope('Broadcast'):\n",
        "            x = F.tile(x[:, np.newaxis, :], [1, dlatent_broadcast, 1])\n",
        "\n",
        "    return F.identity(x)\n",
        "\n",
        "# End of the Mapping Network. "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0p2YqYWxgsFp",
        "colab_type": "code",
        "outputId": "c51a33c8-ae34-4159-c8ce-eae563ecce9b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#Using the Same Input as mentioned in above\n",
        "z1 = latent_z[0]\n",
        "z1 = nn.Variable.from_numpy_array(z1, need_grad=True)\n",
        "print(z1.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1, 512)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VTUWLOdHk7ze",
        "colab_type": "code",
        "outputId": "3865b8be-8ade-4767-87dd-6302118be5b3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        }
      },
      "source": [
        " a1 = G_mapping(z1, None)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "DJ\n",
            "DJ\n",
            "DJ\n",
            "DJ\n",
            "DJ\n",
            "DJ\n",
            "DJ\n",
            "DJ\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BMLkGlhElFUl",
        "colab_type": "code",
        "outputId": "4b05aeff-b94a-4456-ea01-80c439953af4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(a1.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1, 14, 512)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fzszou2Egsxl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Getting the Parameters for Importing the Pre-Trained Weights\n",
        "a2 = nn.get_parameters()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AwtsPrRnlNds",
        "colab_type": "code",
        "outputId": "7bc23c89-b4c0-4680-d8cb-8dd411eeeeb0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "print(a2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "OrderedDict([('G_mapping/FC_0/Dense', <Variable((512, 512), need_grad=True) at 0x7f380096ff98>), ('G_mapping/FC_0/bias', <Variable((512,), need_grad=True) at 0x7f38008bc778>), ('G_mapping/FC_1/Dense', <Variable((512, 512), need_grad=True) at 0x7f38008bc728>), ('G_mapping/FC_1/bias', <Variable((512,), need_grad=True) at 0x7f38008bc6d8>), ('G_mapping/FC_2/Dense', <Variable((512, 512), need_grad=True) at 0x7f38008bc868>), ('G_mapping/FC_2/bias', <Variable((512,), need_grad=True) at 0x7f38008bc598>), ('G_mapping/FC_3/Dense', <Variable((512, 512), need_grad=True) at 0x7f38008bc908>), ('G_mapping/FC_3/bias', <Variable((512,), need_grad=True) at 0x7f38008bc818>), ('G_mapping/FC_4/Dense', <Variable((512, 512), need_grad=True) at 0x7f38008bc9a8>), ('G_mapping/FC_4/bias', <Variable((512,), need_grad=True) at 0x7f38008bc8b8>), ('G_mapping/FC_5/Dense', <Variable((512, 512), need_grad=True) at 0x7f38008bca48>), ('G_mapping/FC_5/bias', <Variable((512,), need_grad=True) at 0x7f38008bc958>), ('G_mapping/FC_6/Dense', <Variable((512, 512), need_grad=True) at 0x7f38008bcae8>), ('G_mapping/FC_6/bias', <Variable((512,), need_grad=True) at 0x7f38008bc9f8>), ('G_mapping/FC_7/Dense', <Variable((512, 512), need_grad=True) at 0x7f38008bcb88>), ('G_mapping/FC_7/bias', <Variable((512,), need_grad=True) at 0x7f38008bca98>)])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hTb9XSLQGmRR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Converting the Bias weights to Nnabla variabla\n",
        "G_map_bias = []\n",
        "for i in G_mapping_bias:\n",
        "    l = nn.Variable.from_numpy_array(i)\n",
        "    G_map_bias.append(l)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tu9k99fEHR53",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Converting the Dense weights to Nnabla variabla\n",
        "G_map_weights = []\n",
        "for i in G_mapping_weights:\n",
        "    l = nn.Variable.from_numpy_array(i)\n",
        "    G_map_weights.append(l)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Roy_ZyI_HnwT",
        "colab_type": "code",
        "outputId": "56325aaa-7906-4deb-c190-fbfffe2e9c94",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Assigning the Pre-Trained weights to the Mapping Network\n",
        "l = 0\n",
        "l1 = 0\n",
        "l2 = 0\n",
        "for i, j in a2.items():\n",
        "    print(i)\n",
        "    if(l%2==0):\n",
        "        j.data.copy_from(G_map_weights[l1].data)\n",
        "        print(\"Value of l1 is:\", l1)\n",
        "        print(\"G_mapping weights shape is:\", G_mapping_weights[l1].shape)\n",
        "        print(\"J shape is:\", j.shape)\n",
        "        print()\n",
        "        l1 = l1 + 1\n",
        "    else:\n",
        "        j.data.copy_from(G_map_bias[l2].data)\n",
        "        print(\"Value of l2 is:\", l2)\n",
        "        print(\"G_mapping bias shape is:\", G_mapping_bias[l2].shape)\n",
        "        print(\"J shape is:\", j.shape)\n",
        "        print()\n",
        "        l2 = l2 + 1\n",
        "    l = l + 1"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "G_mapping/FC_0/Dense\n",
            "Value of l1 is: 0\n",
            "G_mapping weights shape is: (512, 512)\n",
            "J shape is: (512, 512)\n",
            "\n",
            "G_mapping/FC_0/bias\n",
            "Value of l2 is: 0\n",
            "G_mapping bias shape is: (512,)\n",
            "J shape is: (512,)\n",
            "\n",
            "G_mapping/FC_1/Dense\n",
            "Value of l1 is: 1\n",
            "G_mapping weights shape is: (512, 512)\n",
            "J shape is: (512, 512)\n",
            "\n",
            "G_mapping/FC_1/bias\n",
            "Value of l2 is: 1\n",
            "G_mapping bias shape is: (512,)\n",
            "J shape is: (512,)\n",
            "\n",
            "G_mapping/FC_2/Dense\n",
            "Value of l1 is: 2\n",
            "G_mapping weights shape is: (512, 512)\n",
            "J shape is: (512, 512)\n",
            "\n",
            "G_mapping/FC_2/bias\n",
            "Value of l2 is: 2\n",
            "G_mapping bias shape is: (512,)\n",
            "J shape is: (512,)\n",
            "\n",
            "G_mapping/FC_3/Dense\n",
            "Value of l1 is: 3\n",
            "G_mapping weights shape is: (512, 512)\n",
            "J shape is: (512, 512)\n",
            "\n",
            "G_mapping/FC_3/bias\n",
            "Value of l2 is: 3\n",
            "G_mapping bias shape is: (512,)\n",
            "J shape is: (512,)\n",
            "\n",
            "G_mapping/FC_4/Dense\n",
            "Value of l1 is: 4\n",
            "G_mapping weights shape is: (512, 512)\n",
            "J shape is: (512, 512)\n",
            "\n",
            "G_mapping/FC_4/bias\n",
            "Value of l2 is: 4\n",
            "G_mapping bias shape is: (512,)\n",
            "J shape is: (512,)\n",
            "\n",
            "G_mapping/FC_5/Dense\n",
            "Value of l1 is: 5\n",
            "G_mapping weights shape is: (512, 512)\n",
            "J shape is: (512, 512)\n",
            "\n",
            "G_mapping/FC_5/bias\n",
            "Value of l2 is: 5\n",
            "G_mapping bias shape is: (512,)\n",
            "J shape is: (512,)\n",
            "\n",
            "G_mapping/FC_6/Dense\n",
            "Value of l1 is: 6\n",
            "G_mapping weights shape is: (512, 512)\n",
            "J shape is: (512, 512)\n",
            "\n",
            "G_mapping/FC_6/bias\n",
            "Value of l2 is: 6\n",
            "G_mapping bias shape is: (512,)\n",
            "J shape is: (512,)\n",
            "\n",
            "G_mapping/FC_7/Dense\n",
            "Value of l1 is: 7\n",
            "G_mapping weights shape is: (512, 512)\n",
            "J shape is: (512, 512)\n",
            "\n",
            "G_mapping/FC_7/bias\n",
            "Value of l2 is: 7\n",
            "G_mapping bias shape is: (512,)\n",
            "J shape is: (512,)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YI--yZy0IF-e",
        "colab_type": "code",
        "outputId": "ca637528-5bed-45b8-b360-7d7d5feb6e33",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        }
      },
      "source": [
        "l = 0\n",
        "for i, j in a2.items():\n",
        "    if(l==1):\n",
        "        break\n",
        "    l = l + 1\n",
        "    print(i)\n",
        "    print(j[0][0].d)\n",
        "    print(j[0][1].d)\n",
        "    print(j[0][2].d)\n",
        "    print(j[0][3].d)\n",
        "    print(j[0][4].d)\n",
        "    print(j[0][5].d)\n",
        "    print(j[0][6].d)\n",
        "    print(j[0][7].d)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "G_mapping/FC_0/Dense\n",
            "-46.161606\n",
            "-65.11477\n",
            "74.91357\n",
            "-46.699375\n",
            "45.448704\n",
            "-73.10281\n",
            "-139.17113\n",
            "-74.88173\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jt5hZAXRIx7b",
        "colab_type": "code",
        "outputId": "7f398b8e-951d-4ae7-9222-f4e473660687",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        }
      },
      "source": [
        "a12 = G_mapping(z1, None)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "DJ\n",
            "DJ\n",
            "DJ\n",
            "DJ\n",
            "DJ\n",
            "DJ\n",
            "DJ\n",
            "DJ\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "asEvP3DZKmIc",
        "colab_type": "code",
        "outputId": "318135b4-6432-4fcb-f37f-49999c7e6e9c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(a12.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1, 14, 512)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wphpWygTK4h4",
        "colab_type": "code",
        "outputId": "e9be41a0-c1f7-4363-def5-f2b139454748",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        }
      },
      "source": [
        "#Printing the First 10 values of the Output\n",
        "print(a12[0][0][0].d)\n",
        "print(a12[0][0][1].d)\n",
        "print(a12[0][0][2].d)\n",
        "print(a12[0][0][3].d)\n",
        "print(a12[0][0][4].d)\n",
        "print(a12[0][0][5].d)\n",
        "print(a12[0][0][6].d)\n",
        "print(a12[0][0][7].d)\n",
        "print(a12[0][0][8].d)\n",
        "print(a12[0][0][9].d)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.005208131\n",
            "-0.16036135\n",
            "-0.017817566\n",
            "0.0980893\n",
            "-0.03497812\n",
            "0.38442647\n",
            "1.3938818\n",
            "-0.065430276\n",
            "0.049763903\n",
            "-0.07020566\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A5i80DPqL1Ib",
        "colab_type": "code",
        "outputId": "81aad1c9-76c8-4b11-f60f-f55dd66fb52a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        }
      },
      "source": [
        "#Printing the Entire Output\n",
        "print(a12.d)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[[ 0.00520813 -0.16036135 -0.01781757 ... -0.09900362 -0.18152945\n",
            "   -0.21427055]\n",
            "  [ 0.00520813 -0.16036135 -0.01781757 ... -0.09900362 -0.18152945\n",
            "   -0.21427055]\n",
            "  [ 0.00520813 -0.16036135 -0.01781757 ... -0.09900362 -0.18152945\n",
            "   -0.21427055]\n",
            "  ...\n",
            "  [ 0.00520813 -0.16036135 -0.01781757 ... -0.09900362 -0.18152945\n",
            "   -0.21427055]\n",
            "  [ 0.00520813 -0.16036135 -0.01781757 ... -0.09900362 -0.18152945\n",
            "   -0.21427055]\n",
            "  [ 0.00520813 -0.16036135 -0.01781757 ... -0.09900362 -0.18152945\n",
            "   -0.21427055]]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VcNib7ttN7u4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}