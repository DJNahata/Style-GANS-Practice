{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Discriminator Network Check in Tensorflow.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPMj7UtvzK1cguMm4MczVzI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DJNahata/Style-GANS-Practice/blob/master/Discriminator_Network_Check_in_Tensorflow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nqE3l_Q8FEPS",
        "colab_type": "code",
        "outputId": "e2a93e6f-15ba-40bc-e635-c7a136746973",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sy-q09IAFV6M",
        "colab_type": "code",
        "outputId": "aa2f74b1-c1e5-444d-e358-fa6af06ada4f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "!git clone https://github.com/NVlabs/stylegan.git"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'stylegan'...\n",
            "remote: Enumerating objects: 83, done.\u001b[K\n",
            "remote: Total 83 (delta 0), reused 0 (delta 0), pack-reused 83\u001b[K\n",
            "Unpacking objects: 100% (83/83), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UMZ78DxVFXoR",
        "colab_type": "code",
        "outputId": "5d1a3385-f00c-40da-83dc-ce0e5d6d9272",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "cd /content/stylegan/"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/stylegan\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rB3KJYopFZji",
        "colab_type": "code",
        "outputId": "b63434d1-e101-481d-8ebc-4fd12388efb4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        }
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import dnnlib\n",
        "import dnnlib.tflib as tflib\n",
        "\n",
        "# NOTE: Do not import any application-specific modules here!\n",
        "# Specify all network parameters as kwargs.\n",
        "\n",
        "#----------------------------------------------------------------------------\n",
        "# Primitive ops for manipulating 4D activation tensors.\n",
        "# The gradients of these are not necessary efficient or even meaningful.\n",
        "\n",
        "def _blur2d(x, f=[1,2,1], normalize=True, flip=False, stride=1):\n",
        "    assert x.shape.ndims == 4 and all(dim.value is not None for dim in x.shape[1:])\n",
        "    assert isinstance(stride, int) and stride >= 1\n",
        "\n",
        "    # Finalize filter kernel.\n",
        "    f = np.array(f, dtype=np.float32)\n",
        "    if f.ndim == 1:\n",
        "        f = f[:, np.newaxis] * f[np.newaxis, :]\n",
        "    assert f.ndim == 2\n",
        "    if normalize:\n",
        "        f /= np.sum(f)\n",
        "    if flip:\n",
        "        f = f[::-1, ::-1]\n",
        "    f = f[:, :, np.newaxis, np.newaxis]\n",
        "    f = np.tile(f, [1, 1, int(x.shape[1]), 1])\n",
        "\n",
        "    # No-op => early exit.\n",
        "    if f.shape == (1, 1) and f[0,0] == 1:\n",
        "        return x\n",
        "\n",
        "    # Convolve using depthwise_conv2d.\n",
        "    orig_dtype = x.dtype\n",
        "    x = tf.cast(x, tf.float32)  # tf.nn.depthwise_conv2d() doesn't support fp16\n",
        "    f = tf.constant(f, dtype=x.dtype, name='filter')\n",
        "    strides = [1, 1, stride, stride]\n",
        "    x = tf.nn.depthwise_conv2d(x, f, strides=strides, padding='SAME', data_format='NCHW')\n",
        "    x = tf.cast(x, orig_dtype)\n",
        "    return x\n",
        "\n",
        "def _upscale2d(x, factor=2, gain=1):\n",
        "    assert x.shape.ndims == 4 and all(dim.value is not None for dim in x.shape[1:])\n",
        "    assert isinstance(factor, int) and factor >= 1\n",
        "\n",
        "    # Apply gain.\n",
        "    if gain != 1:\n",
        "        x *= gain\n",
        "\n",
        "    # No-op => early exit.\n",
        "    if factor == 1:\n",
        "        return x\n",
        "\n",
        "    # Upscale using tf.tile().\n",
        "    s = x.shape\n",
        "    x = tf.reshape(x, [-1, s[1], s[2], 1, s[3], 1])\n",
        "    x = tf.tile(x, [1, 1, 1, factor, 1, factor])\n",
        "    x = tf.reshape(x, [-1, s[1], s[2] * factor, s[3] * factor])\n",
        "    return x\n",
        "\n",
        "def _downscale2d(x, factor=2, gain=1):\n",
        "    assert x.shape.ndims == 4 and all(dim.value is not None for dim in x.shape[1:])\n",
        "    assert isinstance(factor, int) and factor >= 1\n",
        "\n",
        "    # 2x2, float32 => downscale using _blur2d().\n",
        "    if factor == 2 and x.dtype == tf.float32:\n",
        "        f = [np.sqrt(gain) / factor] * factor\n",
        "        return _blur2d(x, f=f, normalize=False, stride=factor)\n",
        "\n",
        "    # Apply gain.\n",
        "    if gain != 1:\n",
        "        x *= gain\n",
        "\n",
        "    # No-op => early exit.\n",
        "    if factor == 1:\n",
        "        return x\n",
        "\n",
        "    # Large factor => downscale using tf.nn.avg_pool().\n",
        "    # NOTE: Requires tf_config['graph_options.place_pruned_graph']=True to work.\n",
        "    ksize = [1, 1, factor, factor]\n",
        "    return tf.nn.avg_pool(x, ksize=ksize, strides=ksize, padding='VALID', data_format='NCHW')\n",
        "\n",
        "#----------------------------------------------------------------------------\n",
        "# High-level ops for manipulating 4D activation tensors.\n",
        "# The gradients of these are meant to be as efficient as possible.\n",
        "\n",
        "def blur2d(x, f=[1,2,1], normalize=True):\n",
        "    with tf.variable_scope('Blur2D'):\n",
        "        @tf.custom_gradient\n",
        "        def func(x):\n",
        "            y = _blur2d(x, f, normalize)\n",
        "            @tf.custom_gradient\n",
        "            def grad(dy):\n",
        "                dx = _blur2d(dy, f, normalize, flip=True)\n",
        "                return dx, lambda ddx: _blur2d(ddx, f, normalize)\n",
        "            return y, grad\n",
        "        return func(x)\n",
        "\n",
        "def upscale2d(x, factor=2):\n",
        "    with tf.variable_scope('Upscale2D'):\n",
        "        @tf.custom_gradient\n",
        "        def func(x):\n",
        "            y = _upscale2d(x, factor)\n",
        "            @tf.custom_gradient\n",
        "            def grad(dy):\n",
        "                dx = _downscale2d(dy, factor, gain=factor**2)\n",
        "                return dx, lambda ddx: _upscale2d(ddx, factor)\n",
        "            return y, grad\n",
        "        return func(x)\n",
        "\n",
        "def downscale2d(x, factor=2):\n",
        "    with tf.variable_scope('Downscale2D'):\n",
        "        @tf.custom_gradient\n",
        "        def func(x):\n",
        "            y = _downscale2d(x, factor)\n",
        "            @tf.custom_gradient\n",
        "            def grad(dy):\n",
        "                dx = _upscale2d(dy, factor, gain=1/factor**2)\n",
        "                return dx, lambda ddx: _downscale2d(ddx, factor)\n",
        "            return y, grad\n",
        "        return func(x)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n",
            "WARNING:tensorflow:From /content/stylegan/dnnlib/tflib/tfutil.py:34: The name tf.Dimension is deprecated. Please use tf.compat.v1.Dimension instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/stylegan/dnnlib/tflib/tfutil.py:74: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/stylegan/dnnlib/tflib/tfutil.py:128: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wo9OsrUaFc76",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_weight(shape, gain=np.sqrt(2), use_wscale=False, lrmul=1):\n",
        "    fan_in = np.prod(shape[:-1]) # [kernel, kernel, fmaps_in, fmaps_out] or [in, out]\n",
        "    he_std = gain / np.sqrt(fan_in) # He init\n",
        "\n",
        "    # Equalized learning rate and custom learning rate multiplier.\n",
        "    if use_wscale:\n",
        "        init_std = 1.0 / lrmul\n",
        "        runtime_coef = he_std * lrmul\n",
        "    else:\n",
        "        init_std = he_std / lrmul\n",
        "        runtime_coef = lrmul\n",
        "\n",
        "    # Create variable.\n",
        "    init = tf.initializers.random_normal(0, init_std)\n",
        "    return tf.get_variable('weight', shape=shape, initializer=init) * runtime_coef"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h6cBnMOtFf4S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#----------------------------------------------------------------------------\n",
        "# Fully-connected layer.\n",
        "\n",
        "def dense(x, fmaps, res, **kwargs):\n",
        "  with tf.variable_scope('Dense%dx%d' %(2**res, 2**res), reuse = tf.AUTO_REUSE):\n",
        "    if len(x.shape) > 2:\n",
        "        x = tf.reshape(x, [-1, np.prod([d.value for d in x.shape[1:]])])\n",
        "    w = get_weight([x.shape[1].value, fmaps], **kwargs)\n",
        "    w = tf.cast(w, x.dtype)\n",
        "    return tf.matmul(x, w)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5A7XNcIcFiHj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#----------------------------------------------------------------------------\n",
        "# Convolutional layer.\n",
        "\n",
        "def conv2d(x, fmaps, kernel, res, **kwargs):\n",
        "  with tf.variable_scope('Convolution%dx%d' % (2**res,2**res), reuse = tf.AUTO_REUSE):\n",
        "    assert kernel >= 1 and kernel % 2 == 1\n",
        "    w = get_weight([kernel, kernel, x.shape[1].value, fmaps], **kwargs)\n",
        "    w = tf.cast(w, x.dtype)\n",
        "    return tf.nn.conv2d(x, w, strides=[1,1,1,1], padding='SAME', data_format='NCHW')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tgeW1_rTFkNC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#----------------------------------------------------------------------------\n",
        "# Fused convolution + scaling.\n",
        "# Faster and uses less memory than performing the operations separately.\n",
        "\n",
        "def upscale2d_conv2d(x, fmaps, kernel, res, fused_scale='auto', **kwargs):\n",
        "  with tf.variable_scope('upscale2d_conv2d%dx%d' % (2**res,2**res), reuse = tf.AUTO_REUSE):\n",
        "    assert kernel >= 1 and kernel % 2 == 1\n",
        "    assert fused_scale in [True, False, 'auto']\n",
        "    if fused_scale == 'auto':\n",
        "        fused_scale = min(x.shape[2:]) * 2 >= 128\n",
        "\n",
        "    # Not fused => call the individual ops directly.\n",
        "    if not fused_scale:\n",
        "        return conv2d(upscale2d(x), fmaps, kernel, res, **kwargs)\n",
        "\n",
        "    # Fused => perform both ops simultaneously using tf.nn.conv2d_transpose().\n",
        "    w = get_weight([kernel, kernel, x.shape[1].value, fmaps], **kwargs)\n",
        "    w = tf.transpose(w, [0, 1, 3, 2]) # [kernel, kernel, fmaps_out, fmaps_in]\n",
        "    w = tf.pad(w, [[1,1], [1,1], [0,0], [0,0]], mode='CONSTANT')\n",
        "    w = tf.add_n([w[1:, 1:], w[:-1, 1:], w[1:, :-1], w[:-1, :-1]])\n",
        "    w = tf.cast(w, x.dtype)\n",
        "    os = [tf.shape(x)[0], fmaps, x.shape[2] * 2, x.shape[3] * 2]\n",
        "    return tf.nn.conv2d_transpose(x, w, os, strides=[1,1,2,2], padding='SAME', data_format='NCHW')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Npr6-_3PFmlK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def conv2d_downscale2d(x, fmaps, kernel, res, fused_scale='auto', **kwargs):\n",
        "  with tf.variable_scope('conv2d_downscale2d%dx%d' % (2**res,2**res), reuse = tf.AUTO_REUSE):\n",
        "    assert kernel >= 1 and kernel % 2 == 1\n",
        "    assert fused_scale in [True, False, 'auto']\n",
        "    if fused_scale == 'auto':\n",
        "        fused_scale = min(x.shape[2:]) >= 128\n",
        "\n",
        "    # Not fused => call the individual ops directly.\n",
        "    if not fused_scale:\n",
        "        return downscale2d(conv2d(x, fmaps, kernel, res, **kwargs))\n",
        "\n",
        "    # Fused => perform both ops simultaneously using tf.nn.conv2d().\n",
        "    w = get_weight([kernel, kernel, x.shape[1].value, fmaps], **kwargs)\n",
        "    w = tf.pad(w, [[1,1], [1,1], [0,0], [0,0]], mode='CONSTANT')\n",
        "    w = tf.add_n([w[1:, 1:], w[:-1, 1:], w[1:, :-1], w[:-1, :-1]]) * 0.25\n",
        "    w = tf.cast(w, x.dtype)\n",
        "    return tf.nn.conv2d(x, w, strides=[1,1,2,2], padding='SAME', data_format='NCHW')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HPW228GhFozk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#----------------------------------------------------------------------------\n",
        "# Apply bias to the given activation tensor.\n",
        "\n",
        "def apply_bias(x, res, lrmul=1):\n",
        "  with tf.variable_scope('Bias%dx%d' % (2**res,2**res), reuse = tf.AUTO_REUSE):\n",
        "    b = tf.get_variable('bias', shape=[x.shape[1]], initializer=tf.initializers.zeros()) * lrmul\n",
        "    b = tf.cast(b, x.dtype)\n",
        "    if len(x.shape) == 2:\n",
        "        return x + b\n",
        "    return x + tf.reshape(b, [1, -1, 1, 1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZpTzPmuvFq06",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#----------------------------------------------------------------------------\n",
        "# Leaky ReLU activation. More efficient than tf.nn.leaky_relu() and supports FP16.\n",
        "\n",
        "def leaky_relu(x, alpha=0.2):\n",
        "    with tf.variable_scope('LeakyReLU'):\n",
        "        alpha = tf.constant(alpha, dtype=x.dtype, name='alpha')\n",
        "        @tf.custom_gradient\n",
        "        def func(x):\n",
        "            y = tf.maximum(x, x * alpha)\n",
        "            @tf.custom_gradient\n",
        "            def grad(dy):\n",
        "                dx = tf.where(y >= 0, dy, dy * alpha)\n",
        "                return dx, lambda ddx: tf.where(y >= 0, ddx, ddx * alpha)\n",
        "            return y, grad\n",
        "        return func(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rODgMKEyFtYj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#----------------------------------------------------------------------------\n",
        "# Pixelwise feature vector normalization.\n",
        "\n",
        "def pixel_norm(x, epsilon=1e-8):\n",
        "    with tf.variable_scope('PixelNorm'):\n",
        "        epsilon = tf.constant(epsilon, dtype=x.dtype, name='epsilon')\n",
        "        return x * tf.rsqrt(tf.reduce_mean(tf.square(x), axis=1, keepdims=True) + epsilon)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nvM6_k8HFvIC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#----------------------------------------------------------------------------\n",
        "# Instance normalization.\n",
        "\n",
        "def instance_norm(x, epsilon=1e-8):\n",
        "    assert len(x.shape) == 4 # NCHW\n",
        "    with tf.variable_scope('InstanceNorm'):\n",
        "        orig_dtype = x.dtype\n",
        "        x = tf.cast(x, tf.float32)\n",
        "        x -= tf.reduce_mean(x, axis=[2,3], keepdims=True)\n",
        "        epsilon = tf.constant(epsilon, dtype=x.dtype, name='epsilon')\n",
        "        x *= tf.rsqrt(tf.reduce_mean(tf.square(x), axis=[2,3], keepdims=True) + epsilon)\n",
        "        x = tf.cast(x, orig_dtype)\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UdI8jjWoFxFy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#----------------------------------------------------------------------------\n",
        "# Style modulation.\n",
        "\n",
        "def style_mod(x, dlatent, res, **kwargs):\n",
        "    with tf.variable_scope('StyleMod', reuse=tf.AUTO_REUSE):\n",
        "        style = apply_bias(dense(dlatent, fmaps=x.shape[1]*2, res=res, gain=1, **kwargs), res)\n",
        "        style = tf.reshape(style, [-1, 2, x.shape[1]] + [1] * (len(x.shape) - 2))\n",
        "        return x * (style[:,0] + 1) + style[:,1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_MVnM3bqFy-K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#----------------------------------------------------------------------------\n",
        "# Noise input.\n",
        "\n",
        "def apply_noise(x, res, noise_var=None, randomize_noise=True):\n",
        "    assert len(x.shape) == 4 # NCHW\n",
        "    with tf.variable_scope('Noise%dx%d' %(2**res,2**res), reuse=tf.AUTO_REUSE):\n",
        "        if noise_var is None or randomize_noise:\n",
        "            noise = tf.random_normal([tf.shape(x)[0], 1, x.shape[2], x.shape[3]], dtype=x.dtype)\n",
        "        else:\n",
        "            noise = tf.cast(noise_var, x.dtype)\n",
        "        weight = tf.get_variable('weight', shape=[x.shape[1].value], initializer=tf.initializers.zeros())\n",
        "        return x + noise * tf.reshape(tf.cast(weight, x.dtype), [1, -1, 1, 1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QkWfDvxkF0qj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def minibatch_stddev_layer(x, group_size=4, num_new_features=1):\n",
        "    with tf.variable_scope('MinibatchStddev'):\n",
        "        group_size = tf.minimum(group_size, tf.shape(x)[0])     # Minibatch must be divisible by (or smaller than) group_size.\n",
        "        s = x.shape                                             # [NCHW]  Input shape.\n",
        "        y = tf.reshape(x, [group_size, -1, num_new_features, s[1]//num_new_features, s[2], s[3]])   # [GMncHW] Split minibatch into M groups of size G. Split channels into n channel groups c.\n",
        "        y = tf.cast(y, tf.float32)                              # [GMncHW] Cast to FP32.\n",
        "        y -= tf.reduce_mean(y, axis=0, keepdims=True)           # [GMncHW] Subtract mean over group.\n",
        "        y = tf.reduce_mean(tf.square(y), axis=0)                # [MncHW]  Calc variance over group.\n",
        "        y = tf.sqrt(y + 1e-8)                                   # [MncHW]  Calc stddev over group.\n",
        "        y = tf.reduce_mean(y, axis=[2,3,4], keepdims=True)      # [Mn111]  Take average over fmaps and pixels.\n",
        "        y = tf.reduce_mean(y, axis=[2])                         # [Mn11] Split channels into c channel groups\n",
        "        y = tf.cast(y, x.dtype)                                 # [Mn11]  Cast back to original data type.\n",
        "        y = tf.tile(y, [group_size, 1, s[2], s[3]])             # [NnHW]  Replicate over group and pixels.\n",
        "        return tf.concat([x, y], axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1J8Ri4AtF68q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def D_basic(\n",
        "    images_in,                          # First input: Images [minibatch, channel, height, width].\n",
        "    labels_in,                          # Second input: Labels [minibatch, label_size].\n",
        "    num_channels        = 3,            # Number of input color channels. Overridden based on dataset.\n",
        "    resolution          = 256,           # Input resolution. Overridden based on dataset.\n",
        "    label_size          = 0,            # Dimensionality of the labels, 0 if no labels. Overridden based on dataset.\n",
        "    fmap_base           = 8192,         # Overall multiplier for the number of feature maps.\n",
        "    fmap_decay          = 1.0,          # log2 feature map reduction when doubling the resolution.\n",
        "    fmap_max            = 512,          # Maximum number of feature maps in any layer.\n",
        "    nonlinearity        = 'lrelu',      # Activation function: 'relu', 'lrelu',\n",
        "    use_wscale          = True,         # Enable equalized learning rate?\n",
        "    mbstd_group_size    = 4,            # Group size for the minibatch standard deviation layer, 0 = disable.\n",
        "    mbstd_num_features  = 1,            # Number of features for the minibatch standard deviation layer.\n",
        "    dtype               = 'float32',    # Data type to use for activations and outputs.\n",
        "    fused_scale         = 'auto',       # True = fused convolution + scaling, False = separate ops, 'auto' = decide automatically.\n",
        "    blur_filter         = [1,2,1],      # Low-pass filter to apply when resampling activations. None = no filtering.\n",
        "    structure           = 'auto',       # 'fixed' = no progressive growing, 'linear' = human-readable, 'recursive' = efficient, 'auto' = select automatically.\n",
        "    is_template_graph   = False,        # True = template graph constructed by the Network class, False = actual evaluation.\n",
        "    **_kwargs):                         # Ignore unrecognized keyword args.\n",
        "\n",
        "    resolution_log2 = int(np.log2(resolution))\n",
        "    assert resolution == 2**resolution_log2 and resolution >= 4\n",
        "    def nf(stage): return min(int(fmap_base / (2.0 ** (stage * fmap_decay))), fmap_max)\n",
        "    def blur(x): return blur2d(x, blur_filter) if blur_filter else x\n",
        "    if structure == 'auto': structure = 'linear' if is_template_graph else 'recursive'\n",
        "    act, gain = {'relu': (tf.nn.relu, np.sqrt(2)), 'lrelu': (leaky_relu, np.sqrt(2))}[nonlinearity]\n",
        "\n",
        "    images_in.set_shape([None, num_channels, resolution, resolution])\n",
        "\n",
        "    if labels_in is not None:\n",
        "        labels_in.set_shape([None, label_size])\n",
        "        labels_in = tf.cast(labels_in, dtype)\n",
        "\n",
        "    images_in = tf.cast(images_in, dtype)\n",
        "    with tf.variable_scope('lod', reuse = tf.AUTO_REUSE):\n",
        "        lod_in = tf.cast(tf.get_variable('lod', initializer=np.float32(0.0), trainable=False), dtype)\n",
        "\n",
        "\n",
        "    scores_out = None\n",
        "\n",
        "    def fromrgb(x, res): # res = 2..resolution_log2\n",
        "        with tf.variable_scope('FromRGB_lod%d' % (resolution_log2 - res)):\n",
        "            return act(apply_bias(conv2d(x, fmaps=nf(res-1), kernel=1, res = res, gain=gain, use_wscale=use_wscale), res))\n",
        "\n",
        "    def block(x, res): # res = 2..resolution_log2\n",
        "        if res >= 3: # 8x8 and up\n",
        "            with tf.variable_scope('Conv0'):\n",
        "                x = act(apply_bias(conv2d(x, fmaps=nf(res-1), kernel=3, res = res, gain=gain, use_wscale=use_wscale), res))\n",
        "            with tf.variable_scope('Conv1_down'):\n",
        "                x = act(apply_bias(conv2d_downscale2d(blur(x), fmaps=nf(res-2), kernel=3, res = res, gain=gain, use_wscale=use_wscale, fused_scale=fused_scale), res))\n",
        "        else: # 4x4\n",
        "            if mbstd_group_size > 1:\n",
        "                x = minibatch_stddev_layer(x, mbstd_group_size, mbstd_num_features)\n",
        "            with tf.variable_scope('Convol'):\n",
        "                x = act(apply_bias(conv2d(x, fmaps=nf(res-1), kernel=3, res = 2, gain=gain, use_wscale=use_wscale), res))\n",
        "            with tf.variable_scope('Dense0'):\n",
        "                x = act(apply_bias(dense(x, fmaps=nf(res-2), res = 2, gain=gain, use_wscale=use_wscale), 2))\n",
        "            with tf.variable_scope('Dense1'):\n",
        "                x = apply_bias(dense(x, fmaps=max(label_size, 1), res = 2, gain=1, use_wscale=use_wscale), 2)\n",
        "        return x\n",
        "\n",
        "\n",
        "    if structure == 'fixed':\n",
        "        x = fromrgb(images_in, resolution_log2)\n",
        "        for res in range(resolution_log2, 2, -1):\n",
        "            x = block(x, res)\n",
        "        scores_out = block(x, 2)\n",
        "\n",
        "\n",
        "    # Linear structure: simple but inefficient.\n",
        "    if structure == 'linear':\n",
        "        img = images_in\n",
        "        x = fromrgb(img, resolution_log2)\n",
        "        for res in range(resolution_log2, 2, -1):\n",
        "            lod = resolution_log2 - res\n",
        "            x = block(x, res)\n",
        "            img = downscale2d(img)\n",
        "            y = fromrgb(img, res - 1)\n",
        "            with tf.variable_scope('Grow_lod%d' % lod):\n",
        "                x = tflib.lerp_clip(x, y, lod_in - lod)\n",
        "        scores_out = block(x, 2)\n",
        "\n",
        "\n",
        "    # Recursive structure: complex but efficient.\n",
        "    if structure == 'recursive':\n",
        "        def cset(cur_lambda, new_cond, new_lambda):\n",
        "            return lambda: tf.cond(new_cond, new_lambda, cur_lambda)\n",
        "        def grow(res, lod):\n",
        "            x = lambda: fromrgb(downscale2d(images_in, 2**lod), res)\n",
        "            if lod > 0: x = cset(x, (lod_in < lod), lambda: grow(res + 1, lod - 1))\n",
        "            x = block(x(), res); y = lambda: x\n",
        "            if res > 2: y = cset(y, (lod_in > lod), lambda: tflib.lerp(x, fromrgb(downscale2d(images_in, 2**(lod+1)), res - 1), lod_in - lod))\n",
        "            return y()\n",
        "        scores_out = grow(2, resolution_log2 - 2)\n",
        "    \n",
        "    print(scores_out.shape)\n",
        "\n",
        "    # Label conditioning from \"Which Training Methods for GANs do actually Converge?\"\n",
        "    if label_size:\n",
        "        with tf.variable_scope('LabelSwitch'):\n",
        "            scores_out = tf.reduce_sum(scores_out * labels_in, axis=1, keepdims=True)\n",
        "\n",
        "    assert scores_out.dtype == tf.as_dtype(dtype)\n",
        "    scores_out = tf.identity(scores_out, name='scores_out')\n",
        "    return scores_out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lczo-3v3GB6z",
        "colab_type": "code",
        "outputId": "54ce5880-9e1c-4b83-ddf2-f73067859858",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "a1 = np.random.random((1,3,256,256))\n",
        "a1 = tf.convert_to_tensor(a1, None)\n",
        "a2 = D_basic(a1, None)\n",
        "print(a2.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1, 1)\n",
            "(1, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k7ie6-o8lVdQ",
        "colab_type": "code",
        "outputId": "f81983bc-38f2-4849-eb3b-b77566deefd2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "sess = tf.Session()\n",
        "init = tf.initialize_all_variables()\n",
        "sess.run(init)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/util/tf_should_use.py:198: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
            "Instructions for updating:\n",
            "Use `tf.global_variables_initializer` instead.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h11MX26slWY2",
        "colab_type": "code",
        "outputId": "5ed53d7d-a5dd-4013-dcc5-3f5d5ad2b2de",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(sess.run(a2))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.04991315]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TDkqu6wz2fET",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a211 = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='Conv0/Conv')\n",
        "a212 = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='Conv0/Bias')\n",
        "a221 = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='Conv1_down/conv2d_downscale2d')\n",
        "a222 = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='Conv1_down/Bias')\n",
        "a231 = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='Convol/Convolution')\n",
        "a232 = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='Convol/Bias')\n",
        "a241 = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='Dense0/Dense')\n",
        "a242 = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='Dense0/Bias')\n",
        "a251 = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='Dense1/Dense')\n",
        "a252 = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='Dense1/Bias')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xFM9yhAA2fWC",
        "colab_type": "code",
        "outputId": "5abb7523-c0b5-4894-95c1-7591d82a2e58",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        }
      },
      "source": [
        "print(len(a211))\n",
        "print(len(a212))\n",
        "print(len(a221))\n",
        "print(len(a222))\n",
        "print(len(a231))\n",
        "print(len(a232))\n",
        "print(len(a241))\n",
        "print(len(a242))\n",
        "print(len(a251))\n",
        "print(len(a252))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "6\n",
            "6\n",
            "6\n",
            "6\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I9gfx4BRdvK0",
        "colab_type": "code",
        "outputId": "09032898-4098-4c6b-ad30-320b190a7fc2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 689
        }
      },
      "source": [
        "for i in a211:\n",
        "    print(i)\n",
        "print(\" \")\n",
        "for i in a212:\n",
        "    print(i)\n",
        "print()\n",
        "for i in a221:\n",
        "    print(i)\n",
        "print()\n",
        "for i in a222:\n",
        "    print(i)\n",
        "print()\n",
        "print(a231[0])\n",
        "print()\n",
        "print(a232[0])\n",
        "print()\n",
        "print(a241[0])\n",
        "print()\n",
        "print(a242[0])\n",
        "print()\n",
        "print(a251[0])\n",
        "print()\n",
        "print(a252[0])\n",
        "print()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<tf.Variable 'Conv0/Convolution256x256/weight:0' shape=(3, 3, 64, 64) dtype=float32_ref>\n",
            "<tf.Variable 'Conv0/Convolution128x128/weight:0' shape=(3, 3, 128, 128) dtype=float32_ref>\n",
            "<tf.Variable 'Conv0/Convolution64x64/weight:0' shape=(3, 3, 256, 256) dtype=float32_ref>\n",
            "<tf.Variable 'Conv0/Convolution32x32/weight:0' shape=(3, 3, 512, 512) dtype=float32_ref>\n",
            "<tf.Variable 'Conv0/Convolution16x16/weight:0' shape=(3, 3, 512, 512) dtype=float32_ref>\n",
            "<tf.Variable 'Conv0/Convolution8x8/weight:0' shape=(3, 3, 512, 512) dtype=float32_ref>\n",
            " \n",
            "<tf.Variable 'Conv0/Bias256x256/bias:0' shape=(64,) dtype=float32_ref>\n",
            "<tf.Variable 'Conv0/Bias128x128/bias:0' shape=(128,) dtype=float32_ref>\n",
            "<tf.Variable 'Conv0/Bias64x64/bias:0' shape=(256,) dtype=float32_ref>\n",
            "<tf.Variable 'Conv0/Bias32x32/bias:0' shape=(512,) dtype=float32_ref>\n",
            "<tf.Variable 'Conv0/Bias16x16/bias:0' shape=(512,) dtype=float32_ref>\n",
            "<tf.Variable 'Conv0/Bias8x8/bias:0' shape=(512,) dtype=float32_ref>\n",
            "\n",
            "<tf.Variable 'Conv1_down/conv2d_downscale2d256x256/weight:0' shape=(3, 3, 64, 128) dtype=float32_ref>\n",
            "<tf.Variable 'Conv1_down/conv2d_downscale2d128x128/weight:0' shape=(3, 3, 128, 256) dtype=float32_ref>\n",
            "<tf.Variable 'Conv1_down/conv2d_downscale2d64x64/Convolution64x64/weight:0' shape=(3, 3, 256, 512) dtype=float32_ref>\n",
            "<tf.Variable 'Conv1_down/conv2d_downscale2d32x32/Convolution32x32/weight:0' shape=(3, 3, 512, 512) dtype=float32_ref>\n",
            "<tf.Variable 'Conv1_down/conv2d_downscale2d16x16/Convolution16x16/weight:0' shape=(3, 3, 512, 512) dtype=float32_ref>\n",
            "<tf.Variable 'Conv1_down/conv2d_downscale2d8x8/Convolution8x8/weight:0' shape=(3, 3, 512, 512) dtype=float32_ref>\n",
            "\n",
            "<tf.Variable 'Conv1_down/Bias256x256/bias:0' shape=(128,) dtype=float32_ref>\n",
            "<tf.Variable 'Conv1_down/Bias128x128/bias:0' shape=(256,) dtype=float32_ref>\n",
            "<tf.Variable 'Conv1_down/Bias64x64/bias:0' shape=(512,) dtype=float32_ref>\n",
            "<tf.Variable 'Conv1_down/Bias32x32/bias:0' shape=(512,) dtype=float32_ref>\n",
            "<tf.Variable 'Conv1_down/Bias16x16/bias:0' shape=(512,) dtype=float32_ref>\n",
            "<tf.Variable 'Conv1_down/Bias8x8/bias:0' shape=(512,) dtype=float32_ref>\n",
            "\n",
            "<tf.Variable 'Convol/Convolution4x4/weight:0' shape=(3, 3, 513, 512) dtype=float32_ref>\n",
            "\n",
            "<tf.Variable 'Convol/Bias4x4/bias:0' shape=(512,) dtype=float32_ref>\n",
            "\n",
            "<tf.Variable 'Dense0/Dense4x4/weight:0' shape=(8192, 512) dtype=float32_ref>\n",
            "\n",
            "<tf.Variable 'Dense0/Bias4x4/bias:0' shape=(512,) dtype=float32_ref>\n",
            "\n",
            "<tf.Variable 'Dense1/Dense4x4/weight:0' shape=(512, 1) dtype=float32_ref>\n",
            "\n",
            "<tf.Variable 'Dense1/Bias4x4/bias:0' shape=(1,) dtype=float32_ref>\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qa1OtdxU3WWg",
        "colab_type": "code",
        "outputId": "4a82497a-1dea-4955-9e44-abb1305cafbd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "print(a221)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[<tf.Variable 'Conv1_down/conv2d_downscale2d256x256/weight:0' shape=(3, 3, 64, 128) dtype=float32_ref>, <tf.Variable 'Conv1_down/conv2d_downscale2d128x128/weight:0' shape=(3, 3, 128, 256) dtype=float32_ref>, <tf.Variable 'Conv1_down/conv2d_downscale2d64x64/Convolution64x64/weight:0' shape=(3, 3, 256, 512) dtype=float32_ref>, <tf.Variable 'Conv1_down/conv2d_downscale2d32x32/Convolution32x32/weight:0' shape=(3, 3, 512, 512) dtype=float32_ref>, <tf.Variable 'Conv1_down/conv2d_downscale2d16x16/Convolution16x16/weight:0' shape=(3, 3, 512, 512) dtype=float32_ref>, <tf.Variable 'Conv1_down/conv2d_downscale2d8x8/Convolution8x8/weight:0' shape=(3, 3, 512, 512) dtype=float32_ref>]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4O9Jt7Fo3piw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}