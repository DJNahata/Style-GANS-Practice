{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Discriminator Network in Nnabla.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOg81YEg1hSrnAPDJpZkvg1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DJNahata/Style-GANS-Practice/blob/master/Discriminator_Network_in_Nnabla.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C2sOp9T94QpK",
        "colab_type": "code",
        "outputId": "eba4b214-57c7-486a-b465-edcac6f7a944",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 608
        }
      },
      "source": [
        "!pip install nnabla"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting nnabla\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6f/29/e2a05e0c9f0f9f3647fcd4a04bc167bb8bf85d8246581951f0dd3c206457/nnabla-1.7.0-cp36-cp36m-manylinux1_x86_64.whl (14.1MB)\n",
            "\u001b[K     |████████████████████████████████| 14.2MB 235kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from nnabla) (1.18.2)\n",
            "Collecting onnx\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/f4/e126b60d109ad1e80020071484b935980b7cce1e4796073aab086a2d6902/onnx-1.6.0-cp36-cp36m-manylinux1_x86_64.whl (4.8MB)\n",
            "\u001b[K     |████████████████████████████████| 4.8MB 44.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from nnabla) (2.10.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from nnabla) (46.1.3)\n",
            "Requirement already satisfied: imageio in /usr/local/lib/python3.6/dist-packages (from nnabla) (2.4.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from nnabla) (4.38.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from nnabla) (2.21.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from nnabla) (1.4.1)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (from nnabla) (7.0.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nnabla) (1.12.0)\n",
            "Requirement already satisfied: Cython in /usr/local/lib/python3.6/dist-packages (from nnabla) (0.29.16)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from nnabla) (1.12.40)\n",
            "Requirement already satisfied: protobuf>=3.6 in /usr/local/lib/python3.6/dist-packages (from nnabla) (3.10.0)\n",
            "Requirement already satisfied: contextlib2 in /usr/local/lib/python3.6/dist-packages (from nnabla) (0.5.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from nnabla) (3.13)\n",
            "Collecting configparser\n",
            "  Downloading https://files.pythonhosted.org/packages/4b/6b/01baa293090240cf0562cc5eccb69c6f5006282127f2b846fad011305c79/configparser-5.0.0-py3-none-any.whl\n",
            "Requirement already satisfied: typing-extensions>=3.6.2.1 in /usr/local/lib/python3.6/dist-packages (from onnx->nnabla) (3.6.6)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->nnabla) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->nnabla) (2.8)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->nnabla) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->nnabla) (2020.4.5.1)\n",
            "Requirement already satisfied: botocore<1.16.0,>=1.15.40 in /usr/local/lib/python3.6/dist-packages (from boto3->nnabla) (1.15.40)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->nnabla) (0.9.5)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->nnabla) (0.3.3)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.40->boto3->nnabla) (0.15.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.40->boto3->nnabla) (2.8.1)\n",
            "Installing collected packages: onnx, configparser, nnabla\n",
            "Successfully installed configparser-5.0.0 nnabla-1.7.0 onnx-1.6.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9VscHlXC4bM_",
        "colab_type": "code",
        "outputId": "07d197b6-a059-472d-d0be-61f059cdcd49",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "import numpy as np\n",
        "import nnabla as nn\n",
        "import nnabla.parametric_functions as PF\n",
        "import nnabla.functions as F\n",
        "import nnabla.solvers as S\n",
        "import nnabla.logger as logger\n",
        "import nnabla.utils.save as save\n",
        "from nnabla.parameter import get_parameter_or_create\n",
        "from nnabla.ext_utils import get_extension_context\n",
        "from nnabla.parametric_functions import parametric_function_api\n",
        "import nnabla.initializer as I\n",
        "nn.set_auto_forward(True)\n",
        "print(\"DJ1\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-04-21 17:43:12,586 [nnabla][INFO]: Initializing CPU extension...\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "DJ1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4qoZU9vd4jUN",
        "colab_type": "code",
        "outputId": "cc7ab980-8d0d-4689-eea4-6982bf63318e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "from __future__ import absolute_import\n",
        "from six.moves import range\n",
        "print(\"DJ1\")\n",
        "\n",
        "import functools\n",
        "# Globally prepared params.\n",
        "w = nn.parameter.get_parameter_or_create('BLURRING_FILTER', shape=(3,3), need_grad=False)  #why and how is it used for the parameter\n",
        "w.d = np.array([[1., 2., 1.], [2., 4., 2.], [1., 2., 1.]])   \n",
        "\n",
        "print('')\n",
        "\n",
        "\n",
        "def lerp(a, b, t):\n",
        "  if(t>=0):\n",
        "    return ((a + (b - a) * t))\n",
        "  else:\n",
        "    return a\n",
        "\n",
        "def lerp_clip(a, b, t):\n",
        "    return (a + (b - a) * F.clip_by_value(t, 0.0, 1.0))\n",
        "\n",
        "\n",
        "#Computing the mean across the feature maps i.e. axis 2 and 3 for H and W with data format of x as NCHW\n",
        "#--------------------\n",
        "def compute_channel_mean(x, keepdims = True):\n",
        "    ch_mean = F.mean(x, axis = [2,3], keepdims = keepdims)\n",
        "    return ch_mean\n",
        "\n",
        "#--------------------\n",
        "\n",
        "\n",
        "#-------------------------------\n",
        "#Computing the standard deviation across the feature maps i.e. axis 2 and 3 for H and W with data format of x as NCHW\n",
        "def compute_channel_std(x, keepdims = True):\n",
        "    x1 = compute_channel_mean(x)\n",
        "    var = F.mean(F.pow_scalar((x - x1), 2.0), axis = [2,3], keepdims = keepdims)\n",
        "    std = F.pow_scalar(var, 0.5)\n",
        "    return std\n",
        "\n",
        "#-------------------------------\n",
        "\n",
        "\n",
        "#---------------------------------------------------\n",
        "#Adaptive Instance Normalization to be used\n",
        "def adain(x, y):\n",
        "    y_s, y_b = y\n",
        "    ch1_mean = compute_channel_mean(x)\n",
        "    ch1_std = compute_channel_std(x)\n",
        "    x = (x - ch1_mean) / ch1_std\n",
        "    a1 = (y_s * x) + y_b\n",
        "    return a1 \n",
        "\n",
        "#-------------------------------\n",
        "\n",
        "\n",
        "#New blur function to be used ------------------------------------------\n",
        "def blur2d(x, f=[1,2,1], normalize = True, flip = False, stride = 1):\n",
        "    ## assert x.ndim == 4 and all(dim.value is not None for dim in x.shape[1:])\n",
        "    assert isinstance(stride, int) and stride >= 1\n",
        "\n",
        "    f = np.array(f, dtype = np.float32)\n",
        "\n",
        "    if f.ndim == 1:\n",
        "        f = f[:, np.newaxis] * f[np.newaxis, :]\n",
        "\n",
        "    if flip:\n",
        "        f = f[::-1, ::-1]\n",
        "\n",
        "    if normalize:\n",
        "        f = f / np.sum(f)\n",
        "\n",
        "    f = f[ np.newaxis, :, :]\n",
        "    f = np.tile(f, [int(x.shape[1]), 1, 1])\n",
        "    if f.shape == (1, 1) and f[0,0] == 1:\n",
        "        return x\n",
        "\n",
        "    #orig_dtype = x.dtype\n",
        "    strides = (stride, stride) \n",
        "    w = nn.Variable.from_numpy_array(f)\n",
        "    padding = (w.shape[1] // 2, w.shape[2] // 2)\n",
        "\n",
        "    y = F.depthwise_convolution(x, w, pad = padding, stride = strides)\n",
        "    return y"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "DJ1\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p4wq_4jd4m6t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_weight(he_std, gain = np.sqrt(2), use_wscale = False, lrmul = 1):\n",
        "    if use_wscale:\n",
        "        init_std = 1.0 / lrmul\n",
        "        runtime_coeff = he_std * lrmul\n",
        "    else: \n",
        "        init_std = he_std / lrmul\n",
        "        runtime_coeff = lrmul\n",
        "\n",
        "    w_init = I.NormalInitializer(sigma = init_std)   #I.NormalInitializer function is used with sigma argument as the standard deviation, so as to get the weights of these variables \n",
        "    return w_init, runtime_coeff"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-k-RTnYN4pq1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def downscale2d(x, factor = 2, gain = 1):\n",
        "    #assert x.ndim == 4 and all(dim.value is not None for dim in x.shape[1:])\n",
        "    assert isinstance(factor, int) and factor >=1\n",
        "\n",
        "    if factor == 2:\n",
        "        f = [np.sqrt(gain) / factor] * factor\n",
        "        f = np.array(f, dtype = np.float32)\n",
        "        if f.ndim == 1:\n",
        "            f = f[:, np.newaxis] * f[np.newaxis, :]\n",
        "        assert f.ndim == 2\n",
        "\n",
        "        f = f[np.newaxis, :, :]\n",
        "        f = np.tile(f, [int(x.shape[1]), 1, 1])\n",
        "        w = nn.Variable.from_numpy_array(f)\n",
        "        strides = (factor, factor)\n",
        "        padding = (w.shape[1] // 2, w.shape[2] // 2)\n",
        "        \n",
        "        y = F.depthwise_convolution(x, w, stride = strides)\n",
        "        return y \n",
        "    \n",
        "    if gain != 1:\n",
        "        x = x * gain\n",
        "\n",
        "    if factor == 1:\n",
        "        return x\n",
        "\n",
        "    kernel = [factor, factor]\n",
        "    y = F.average_pooling(x, kernel = kernel, stride = kernel)    # a bit of doubt regarding this statement\n",
        "    return y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PxEgXxOy4sKe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def dense(x, fmaps, gain = np.sqrt(2), use_wscale = False, lrmul = 1, **kwargs):\n",
        "    if len(x.shape) > 2:\n",
        "        x = F.reshape(x, [-1, np.prod([d for d in x.shape[1:]])])\n",
        "    he_std = I.calc_normal_std_he_forward(x.shape[1], fmaps)\n",
        "    w_init, runtime_coeff = get_weight(he_std, gain, use_wscale, lrmul, **kwargs)\n",
        "    w = nn.parameter.get_parameter_or_create('Dense', shape = (x.shape[1], fmaps), initializer = w_init, need_grad = True)\n",
        "    w = w * runtime_coeff\n",
        "    return (F.affine(x, w, bias=None, base_axis=1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q8hXBor34uP9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Upscale operation function to be used \n",
        "def upscale2d(x, factor = 2, gain = 1):\n",
        "    # to check whether the shape of the x is 4 or not\n",
        "    #assert x.ndim == 4 and all(dim.value is not None for dim in x.shape[1:])\n",
        "    assert isinstance(factor, int) and factor >=1 \n",
        "\n",
        "    if gain!=1:\n",
        "        x = x * gain\n",
        "    \n",
        "    if factor == 1:\n",
        "        return x\n",
        "\n",
        "    s = x.shape\n",
        "    #reshape and tile function similar as that of the tensorflow\n",
        "    x = F.reshape(x, [-1, s[1], s[2], 1, s[3], 1])\n",
        "    x = F.tile(x, [1, 1, 1, factor, 1, factor])\n",
        "    x = F.reshape(x, [-1, s[1], s[2] * factor, s[3] * factor])\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ByMNJL974wY-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def conv2d(x, fmaps, kernel, gain = np.sqrt(2), use_wscale = False, **kwargs):\n",
        "  with nn.parameter_scope('Convolution'):\n",
        "    assert isinstance(kernel, int) or isinstance(kernel, float)\n",
        "    assert kernel >= 1 and kernel % 2 == 1\n",
        "    if kernel == 1:\n",
        "        padding = None\n",
        "    else:\n",
        "        padding = ((kernel - 2), (kernel - 2))  #tuple of padding\n",
        "    kernel_tuple = (kernel, kernel)  #tuple of kernel_tuple\n",
        "    he_std = I.calc_normal_std_he_forward(fmaps, x.shape[1], kernel_tuple)        # (kernel, kernel, x.shape[1], fmaps) (3, 2, 0, 1)\n",
        "    w_init, runtime_coeff = get_weight(he_std, gain, use_wscale, **kwargs)\n",
        "    return PF.convolution(x, fmaps, kernel = kernel_tuple, w_init = w_init, with_bias = False, pad = padding, apply_w = lambda w: w * runtime_coeff)\n",
        "\n",
        "#usage of the Noise application which is used in the synthesis network.\n",
        "def apply_noise(x, noise_var = None, randomize_noise = True):\n",
        "    assert len(x.shape) == 4\n",
        "    with nn.parameter_scope('Noise'):\n",
        "        if noise_var is None or randomize_noise:\n",
        "            noise = F.randn(shape = (x.shape[0], 1, x.shape[2], x.shape[3]))\n",
        "        else:\n",
        "            print(\"Dj1\")\n",
        "            noise = noise_var\n",
        "        weight = nn.parameter.get_parameter_or_create(name = 'weight', shape = [x.shape[1]], initializer=I.ConstantInitializer(0))     # For Creating Random nnabla variable such that each entry follows the Normal Distribution..\n",
        "        weight = F.reshape(weight, [1, -1, 1, 1], inplace= False)\n",
        "        return (x + noise * weight)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0F9hGwp64ycv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def upscale2d_conv2d(x, fmaps, kernel, gain = np.sqrt(2), use_wscale = False, fused_scale = 'auto', **kwargs):\n",
        "    assert kernel >= 1 and kernel % 2 == 1\n",
        "    assert fused_scale in [True, False, 'auto']\n",
        "    if fused_scale == 'auto':\n",
        "        fused_scale = min(x.shape[2:]) * 2 >= 128\n",
        "\n",
        "    if not fused_scale:\n",
        "        return conv2d(upscale2d(x), fmaps, kernel, **kwargs)\n",
        "\n",
        "    kernel_tuple = (kernel, kernel)\n",
        "    he_std = I.calc_normal_std_he_forward(x.shape[1], fmaps, kernel_tuple)        # (kernel, kernel, x.shape[1], fmaps)  (2, 3, 0, 1)\n",
        "    w_init, runtime_coeff = get_weight(he_std, gain, use_wscale, **kwargs)\n",
        "    w = nn.parameter.get_parameter_or_create('deconv/W', shape = (x.shape[1], fmaps, kernel, kernel), initializer = w_init, need_grad = True)\n",
        "    w = w * runtime_coeff\n",
        "    w = F.pad(w, (1, 1, 1, 1), mode = 'constant')\n",
        "    w = w[..., 1:, 1:] + w[..., :-1, 1:] + w[..., 1:, :-1] + w[..., :-1, :-1]\n",
        "    kernel_size = w.shape[-1]\n",
        "    padding = ((kernel_size - 2) / 2, (kernel_size - 2) / 2)\n",
        "    stride = (2 , 2)\n",
        "    return F.deconvolution(x, w, bias = None, pad = padding, stride = stride)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AZ4n1uU641ug",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Change in the apply_bias function.\n",
        "def apply_bias(x, lrmul = 1):\n",
        "    b = nn.parameter.get_parameter_or_create(name = 'bias', shape = [x.shape[1]], initializer = I.ConstantInitializer(0))   # Change the initializer to either float or convert the datatype to float32.\n",
        "    b = b * lrmul  # No Modifications requried..\n",
        "    if x.ndim == 2:\n",
        "        return (x + F.reshape(b, [1, -1]))\n",
        "    return (x + F.reshape(b, [1, -1, 1, 1])) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "96lKkYt2430n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#defining the style modulation i.e. combining ys and yb with the resultant of the addition of the noise and the feature maps\n",
        "def style_mod(x, dlatent, **kwargs):\n",
        "    with nn.parameter_scope('StyleMod'):\n",
        "        style = apply_bias(dense(dlatent, fmaps = x.shape[1] * 2, **kwargs))\n",
        "        style = F.reshape(style, [-1, 2, x.shape[1]] + [1] * (len(x.shape) - 2))\n",
        "        return x * (style[:,0] + 1) + style[:,1]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W2XMu42L45tw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def conv2d_downscale2d(x, fmaps, kernel, gain = np.sqrt(2), use_wscale = False, fused_scale = 'auto', **kwargs):\n",
        "    #assert kernel % 2 == 1 and kernel >= 1\n",
        "    assert fused_scale in [True, False, 'auto']\n",
        "    if fused_scale == 'auto':\n",
        "        fused_scale = min(x.shape[2:]) >= 128   #why is this written and what is the condition behind that\n",
        "\n",
        "    if not fused_scale:\n",
        "        return downscale2d(conv2d(x, fmaps, kernel, **kwargs))\n",
        "\n",
        "    kernel_tuple = (kernel, kernel)\n",
        "    he_std = I.calc_normal_std_he_forward(fmaps, x.shape[1], kernel_tuple)       # (kernel, kernel, x.shape[1], fmaps) (3, 2, 0, 1)\n",
        "    w_init, runtime_coeff = get_weight(he_std, gain = gain, use_wscale = use_wscale, **kwargs)\n",
        "    w = nn.parameter.get_parameter_or_create('conv/W', shape = (fmaps, x.shape[1], kernel, kernel), initializer = w_init, need_grad = True)\n",
        "    w = w * runtime_coeff\n",
        "    w = F.pad(w, (1, 1, 1, 1), mode = 'constant')\n",
        "    w = w[..., 1:, 1:] + w[..., :-1, 1:] + w[..., 1:, :-1] + w[..., :-1, :-1]\n",
        "    w = w * 0.25\n",
        "    kernel_size = w.shape[-1]\n",
        "    padding = ((kernel_size - 2) / 2, (kernel_size - 2) / 2)\n",
        "    stride = (2,2)\n",
        "    return F.convolution(x, w, bias=None, pad = padding, stride = stride)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YS439WvA479I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def minibatch_stddev_layer(x, group_size=4, num_new_features=1):\n",
        "    group_size = min(group_size, x.shape[0])     # Minibatch must be divisible by (or smaller than) group_size.\n",
        "    s = x.shape                                             # [NCHW]  Input shape.\n",
        "    y = F.reshape(x, [group_size, -1, num_new_features, s[1]//num_new_features, s[2], s[3]])   # [GMncHW] Split minibatch into M groups of size G. Split channels into n channel groups c.\n",
        "    y -= F.mean(y, axis=0, keepdims=True)           # [GMncHW] Subtract mean over group.\n",
        "    y = F.mean(F.pow_scalar(y, 2.0), axis=0)                # [MncHW]  Calc variance over group.\n",
        "    y = F.add_scalar(F.pow_scalar(y, 2.0) + 1e-8)                                   # [MncHW]  Calc stddev over group.\n",
        "    y = F.mean(y, axis=[2,3,4], keepdims=True)      # [Mn111]  Take average over fmaps and pixels.\n",
        "    y = F.mean(y, axis=[2])                         # [Mn11] Split channels into c channel groups\n",
        "    y = F.tile(y, [group_size, 1, s[2], s[3]])             # [NnHW]  Replicate over group and pixels.\n",
        "    return F.concatenate(x, y, axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H1jk--6o5IEZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#--------------------------------------\n",
        "#application of the activation function\n",
        "def leaky_relu(x):\n",
        "    return functools.partial(F.leaky_relu, alpha = 0.2)(x)\n",
        "#--------------------------------------\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d-5Q-mUy5KEy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pixel_norm(x, epsilon = 1e-8):\n",
        "    mean1 = F.mean(F.pow_scalar(x, 2.0), axis = 1, keepdims = True)\n",
        "    new_mean1 = F.add_scalar(mean1, epsilon)\n",
        "    denominator = F.pow_scalar(new_mean1, 0.5)\n",
        "    x = F.div2(x, denominator)\n",
        "    return x\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lRzWDWg55Mmw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def instance_norm(x, epsilon = 1e-8):\n",
        "    assert len(x.shape) == 4\n",
        "    with nn.parameter_scope('InstanceNorm'):\n",
        "        x1 = compute_channel_mean(x)\n",
        "        x = x - x1\n",
        "        x2 = compute_channel_mean(F.add_scalar(F.pow_scalar(x , 2.0), epsilon))\n",
        "        x2 = F.pow_scalar(x2, 0.5)\n",
        "        x = x / x2\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "syaeMcCh5O1R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def D_basic(\n",
        "    images_in,                          # First input: Images [minibatch, channel, height, width].\n",
        "    #labels_in,                          # Second input: Labels [minibatch, label_size].\n",
        "    num_channels        = 3,            # Number of input color channels. Overridden based on dataset.\n",
        "    resolution          = 256,           # Input resolution. Overridden based on dataset.\n",
        "    label_size          = 0,            # Dimensionality of the labels, 0 if no labels. Overridden based on dataset.\n",
        "    fmap_base           = 8192,         # Overall multiplier for the number of feature maps.\n",
        "    fmap_decay          = 1.0,          # log2 feature map reduction when doubling the resolution.\n",
        "    fmap_max            = 512,          # Maximum number of feature maps in any layer.\n",
        "    nonlinearity        = 'lrelu',      # Activation function: 'relu', 'lrelu',\n",
        "    use_wscale          = True,         # Enable equalized learning rate?\n",
        "    mbstd_group_size    = 4,            # Group size for the minibatch standard deviation layer, 0 = disable.\n",
        "    mbstd_num_features  = 1,            # Number of features for the minibatch standard deviation layer.\n",
        "    dtype               = 'float32',    # Data type to use for activations and outputs.\n",
        "    fused_scale         = 'auto',       # True = fused convolution + scaling, False = separate ops, 'auto' = decide automatically.\n",
        "    blur_filter         = [1,2,1],      # Low-pass filter to apply when resampling activations. None = no filtering.\n",
        "    structure           = 'linear',       # 'fixed' = no progressive growing, 'linear' = human-readable, 'recursive' = efficient, 'auto' = select automatically.\n",
        "    is_template_graph   = False,        # True = template graph constructed by the Network class, False = actual evaluation.\n",
        "    **_kwargs):                         # Ignore unrecognized keyword args.\n",
        "\n",
        "    resolution_log2 = int(np.log2(resolution))\n",
        "    assert resolution == 2**resolution_log2 and resolution >= 4\n",
        "\n",
        "    def nf(stage): return min(int(fmap_base / (2.0 ** (stage * fmap_decay))), fmap_max)\n",
        "\n",
        "    def blur(x):\n",
        "        return blur2d(x, blur_filter) if blur_filter else x\n",
        "\n",
        "    if structure == 'auto':\n",
        "        structure = 'linear' if is_template_graph else 'recursive'\n",
        "\n",
        "    act, gain = {'relu': (F.relu, np.sqrt(2)), 'lrelu': (leaky_relu, np.sqrt(2))}[nonlinearity]\n",
        "\n",
        "    #lod_in = tf.get_variable('lod', initializer=np.float32(0.0), trainable=False)\n",
        "    lod_in = 0\n",
        "    scores_out = None\n",
        "\n",
        "    # Building blocks.\n",
        "    def fromrgb(x, res):  # res = 2..resolution_log2\n",
        "        with nn.parameter_scope('FromRGB_lod{}'.format(resolution_log2 - res)):\n",
        "            return act(apply_bias(conv2d(x, fmaps=nf(res-1), kernel=1, gain=gain, use_wscale=use_wscale)))\n",
        "\n",
        "    def block(x, res):  # res = 2..resolution_log2\n",
        "        with nn.parameter_scope('{}x{}'.format(2**res, 2**res)):\n",
        "            if res >= 3:  # 8x8 and up\n",
        "                with nn.parameter_scope('Conv0'):\n",
        "                    x = act(apply_bias(conv2d(x, fmaps=nf(res-1), kernel=3, gain=gain, use_wscale=use_wscale)))\n",
        "                with nn.parameter_scope('Conv1_down'):\n",
        "                    x = act(apply_bias(conv2d_downscale2d(blur(x), fmaps=nf(res-2), kernel=3, gain=gain, use_wscale=use_wscale, fused_scale=fused_scale)))\n",
        "            else:  # 4x4\n",
        "                if mbstd_group_size > 1:\n",
        "                    x = minibatch_stddev_layer(x, mbstd_group_size, mbstd_num_features)\n",
        "                with nn.parameter_scope('Conv'):\n",
        "                    x = act(apply_bias(conv2d(x, fmaps=nf(res-1), kernel=3, gain=gain, use_wscale=use_wscale)))\n",
        "                with nn.parameter_scope('Dense0'):\n",
        "                    x = act(apply_bias(dense(x, fmaps=nf(res-2), gain=gain, use_wscale=use_wscale)))\n",
        "                with nn.parameter_scope('Dense1'):\n",
        "                    x = apply_bias(dense(x, fmaps=max(label_size, 1), gain=1, use_wscale=use_wscale))\n",
        "            return x\n",
        "\n",
        "    # Fixed structure: simple and efficient, but does not support progressive growing.\n",
        "    if structure == 'fixed':\n",
        "        x = fromrgb(images_in, resolution_log2)\n",
        "        for res in range(resolution_log2, 2, -1):\n",
        "            x = block(x, res)\n",
        "        scores_out = block(x, 2)\n",
        "\n",
        "    \n",
        "    # Linear structure: simple but inefficient.\n",
        "    if structure == 'linear':\n",
        "        img = images_in\n",
        "        x = fromrgb(img, resolution_log2)\n",
        "        for res in range(resolution_log2, 2, -1):\n",
        "            lod = resolution_log2 - res\n",
        "            x = block(x, res)\n",
        "            img = downscale2d(img)\n",
        "            y = fromrgb(img, res - 1)\n",
        "            with nn.parameter_scope('Grow_lod%d' % lod):\n",
        "                x = lerp(x, y, lod_in - lod)\n",
        "        scores_out = block(x, 2)\n",
        "\n",
        "    # Recursive structure: complex but efficient.\n",
        "    if structure == 'recursive':\n",
        "        def cset(cur_lambda, new_cond, new_lambda):\n",
        "            return lambda: tf.cond(new_cond, new_lambda, cur_lambda)\n",
        "        def grow(res, lod):\n",
        "            x = lambda: fromrgb(downscale2d(images_in, 2**lod), res)\n",
        "            if lod > 0: x = cset(x, (lod_in < lod), lambda: grow(res + 1, lod - 1))\n",
        "            x = block(x(), res); y = lambda: x\n",
        "            if res > 2: y = cset(y, (lod_in > lod), lambda: tflib.lerp(x, fromrgb(downscale2d(images_in, 2**(lod+1)), res - 1), lod_in - lod))\n",
        "            return y()\n",
        "        scores_out = grow(2, resolution_log2 - 2)\n",
        "    \n",
        "\n",
        "    # Label conditioning from \"Which Training Methods for GANs do actually Converge?\"\n",
        "    if label_size:\n",
        "        with nn.parameter_scope('LabelSwitch'):\n",
        "            scores_out = F.sum(scores_out * labels_in, axis=1, keepdims=True)\n",
        "\n",
        "    scores_out = F.identity(scores_out)\n",
        "    return scores_out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TmlbOPMV59AM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "i1 = np.random.random((1,3,256,256))\n",
        "i2 = nn.Variable.from_numpy_array(i1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBQ4UCNz6Kv6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "output1 = D_basic(i2, use_wscale = False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1p3KVQJi6Qtk",
        "colab_type": "code",
        "outputId": "62408fbb-923b-40e4-9ecd-1726624f5dd6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(output1.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cv6b_ROf9goO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle\n",
        "with open(\"we1.txt\", \"rb\") as fp:   \n",
        "   we1 = pickle.load(fp)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jxF8PB8N9hL7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open(\"we2.txt\", \"rb\") as fp:   \n",
        "   we2 = pickle.load(fp)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GpL6M5pv_FaN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open(\"we3.txt\", \"rb\") as fp:   \n",
        "   we3 = pickle.load(fp)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u4Kxld_F_Fed",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open(\"we4.txt\", \"rb\") as fp:   \n",
        "   we4 = pickle.load(fp)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iKZ4iWw2_Fhe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open(\"we5.txt\", \"rb\") as fp:   \n",
        "   we5 = pickle.load(fp)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "woSyR_vO_Fwx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open(\"we6.txt\", \"rb\") as fp:   \n",
        "   we6 = pickle.load(fp)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uC0gOXsO_Fc8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open(\"we7.txt\", \"rb\") as fp:   \n",
        "   we7 = pickle.load(fp)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6EjQIDGx9GDC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ax = nn.get_parameters()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NAli4Wdw9JOx",
        "colab_type": "code",
        "outputId": "7ded42f9-49e1-4667-f3c1-4c982e288843",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 756
        }
      },
      "source": [
        "l = 0\n",
        "for i, j in ax.items():\n",
        "    print(i)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "FromRGB_lod0/Convolution/conv/W\n",
            "FromRGB_lod0/bias\n",
            "256x256/Conv0/Convolution/conv/W\n",
            "256x256/Conv0/bias\n",
            "256x256/Conv1_down/conv/W\n",
            "256x256/Conv1_down/bias\n",
            "FromRGB_lod1/Convolution/conv/W\n",
            "FromRGB_lod1/bias\n",
            "128x128/Conv0/Convolution/conv/W\n",
            "128x128/Conv0/bias\n",
            "128x128/Conv1_down/conv/W\n",
            "128x128/Conv1_down/bias\n",
            "FromRGB_lod2/Convolution/conv/W\n",
            "FromRGB_lod2/bias\n",
            "64x64/Conv0/Convolution/conv/W\n",
            "64x64/Conv0/bias\n",
            "64x64/Conv1_down/Convolution/conv/W\n",
            "64x64/Conv1_down/bias\n",
            "FromRGB_lod3/Convolution/conv/W\n",
            "FromRGB_lod3/bias\n",
            "32x32/Conv0/Convolution/conv/W\n",
            "32x32/Conv0/bias\n",
            "32x32/Conv1_down/Convolution/conv/W\n",
            "32x32/Conv1_down/bias\n",
            "FromRGB_lod4/Convolution/conv/W\n",
            "FromRGB_lod4/bias\n",
            "16x16/Conv0/Convolution/conv/W\n",
            "16x16/Conv0/bias\n",
            "16x16/Conv1_down/Convolution/conv/W\n",
            "16x16/Conv1_down/bias\n",
            "FromRGB_lod5/Convolution/conv/W\n",
            "FromRGB_lod5/bias\n",
            "8x8/Conv0/Convolution/conv/W\n",
            "8x8/Conv0/bias\n",
            "8x8/Conv1_down/Convolution/conv/W\n",
            "8x8/Conv1_down/bias\n",
            "FromRGB_lod6/Convolution/conv/W\n",
            "FromRGB_lod6/bias\n",
            "4x4/Conv/Convolution/conv/W\n",
            "4x4/Conv/bias\n",
            "4x4/Dense0/Dense\n",
            "4x4/Dense0/bias\n",
            "4x4/Dense1/Dense\n",
            "4x4/Dense1/bias\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T9ZXlnhO9YhT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "l = 0\n",
        "l1 = 0\n",
        "for i, j in ax.items():\n",
        "    if(l==6):\n",
        "        break\n",
        "    if(l>=0):\n",
        "        if(l%2==0):\n",
        "            a1 = np.transpose(we1[l1], axes = [3, 2, 0, 1])\n",
        "            j.data.copy_from(nn.Variable.from_numpy_array(a1).data)\n",
        "        else:\n",
        "            j.data.copy_from(nn.Variable.from_numpy_array(we1[l1]).data)\n",
        "        l1 = l1 + 1\n",
        "    l = l + 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l0OukALC_WDy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "l = 0\n",
        "l1 = 0\n",
        "for i, j in ax.items():\n",
        "    if(l==12):\n",
        "        break\n",
        "    if(l>=6):\n",
        "        if(l%2==0):\n",
        "            a1 = np.transpose(we2[l1], axes = [3, 2, 0, 1])\n",
        "            j.data.copy_from(nn.Variable.from_numpy_array(a1).data)\n",
        "        else:\n",
        "            j.data.copy_from(nn.Variable.from_numpy_array(we2[l1]).data)\n",
        "        l1 = l1 + 1\n",
        "    l = l + 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YrimzQ1g_ixI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "l = 0\n",
        "l1 = 0\n",
        "for i, j in ax.items():\n",
        "    if(l==18):\n",
        "        break\n",
        "    if(l>=12):\n",
        "        if(l%2==0):\n",
        "            a1 = np.transpose(we3[l1], axes = [3, 2, 0, 1])\n",
        "            j.data.copy_from(nn.Variable.from_numpy_array(a1).data)\n",
        "        else:\n",
        "            j.data.copy_from(nn.Variable.from_numpy_array(we3[l1]).data)\n",
        "        l1 = l1 + 1\n",
        "    l = l + 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ogeOkSf_qLQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "l = 0\n",
        "l1 = 0\n",
        "for i, j in ax.items():\n",
        "    if(l==24):\n",
        "        break\n",
        "    if(l>=18):\n",
        "        if(l%2==0):\n",
        "            a1 = np.transpose(we4[l1], axes = [3, 2, 0, 1])\n",
        "            j.data.copy_from(nn.Variable.from_numpy_array(a1).data)\n",
        "        else:\n",
        "            j.data.copy_from(nn.Variable.from_numpy_array(we4[l1]).data)\n",
        "        l1 = l1 + 1\n",
        "    l = l + 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4b-pmYy5_vMY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "l = 0\n",
        "l1 = 0\n",
        "for i, j in ax.items():\n",
        "    if(l==30):\n",
        "        break\n",
        "    if(l>=24):\n",
        "        if(l%2==0):\n",
        "            a1 = np.transpose(we5[l1], axes = [3, 2, 0, 1])\n",
        "            j.data.copy_from(nn.Variable.from_numpy_array(a1).data)\n",
        "        else:\n",
        "            j.data.copy_from(nn.Variable.from_numpy_array(we5[l1]).data)\n",
        "        l1 = l1 + 1\n",
        "    l = l + 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "32Zm5rKM_0kn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "l = 0\n",
        "l1 = 0\n",
        "for i, j in ax.items():\n",
        "    if(l==36):\n",
        "        break\n",
        "    if(l>=30):\n",
        "        if(l%2==0):\n",
        "            a1 = np.transpose(we6[l1], axes = [3, 2, 0, 1])\n",
        "            j.data.copy_from(nn.Variable.from_numpy_array(a1).data)\n",
        "        else:\n",
        "            j.data.copy_from(nn.Variable.from_numpy_array(we6[l1]).data)\n",
        "        l1 = l1 + 1\n",
        "    l = l + 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yq09jQxC_9BR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "l = 0\n",
        "l1 = 0\n",
        "for i, j in ax.items():\n",
        "    if(l==40):\n",
        "        break\n",
        "    if(l>=36):\n",
        "        if(l%2==0):\n",
        "            a1 = np.transpose(we7[l1], axes = [3, 2, 0, 1])\n",
        "            j.data.copy_from(nn.Variable.from_numpy_array(a1).data)\n",
        "        else:\n",
        "            j.data.copy_from(nn.Variable.from_numpy_array(we7[l1]).data)\n",
        "        l1 = l1 + 1\n",
        "    l = l + 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8KDoVc8tAJVF",
        "colab_type": "code",
        "outputId": "882cdc29-c40a-4438-b972-af86af9ef30c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        }
      },
      "source": [
        "l = 0\n",
        "l1 = 4\n",
        "for i, j in ax.items():\n",
        "    if(l==44):\n",
        "        break\n",
        "    if(l>=40):\n",
        "        if(l%2==0):\n",
        "            a1 = we7[l1]\n",
        "            print(a1.shape)\n",
        "            print(j.shape)\n",
        "            print()\n",
        "            j.data.copy_from(nn.Variable.from_numpy_array(a1).data)\n",
        "        else:\n",
        "            j.data.copy_from(nn.Variable.from_numpy_array(we7[l1]).data)\n",
        "        l1 = l1 + 1\n",
        "    l = l + 1"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(8192, 512)\n",
            "(8192, 512)\n",
            "\n",
            "(512, 1)\n",
            "(512, 1)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xysHKcuTAVq3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open(\"inpu.txt\", \"rb\") as fp:   \n",
        "   inpu = pickle.load(fp)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zo-1SLheBN33",
        "colab_type": "code",
        "outputId": "262cfcec-3167-413b-d18f-98b5ac665a31",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "input1 = inpu[0]\n",
        "input1 = nn.Variable.from_numpy_array(input1)\n",
        "print(input1.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1, 3, 256, 256)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5c6gVNR0BUml",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "output2 = D_basic(input1, use_wscale = False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yn2qrK4WBeqe",
        "colab_type": "code",
        "outputId": "66a5fe6e-c191-469a-c359-64b90e252ddf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(output2.d)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[-3.3796804e+24]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G3tj-Wm-BgV7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}